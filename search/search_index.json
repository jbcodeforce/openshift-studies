{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kubernetes Introduction","text":"<p>Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.  It groups containers that make up an application into logical units for easy management and discovery.</p>"},{"location":"#kubernetes-summary","title":"Kubernetes summary","text":""},{"location":"#openshift-specifics","title":"OpenShift specifics","text":""},{"location":"ansible/","title":"Ansible","text":"<p>Red Hat Ansible Automation Platform fits in the infrastructure and development automation categories, with development, DevOps, compute, network, storage, applications, security, and Internet of Things.</p>"},{"location":"ansible/#features-set","title":"Features set","text":"<ul> <li>YAML based configuration</li> <li>Agentless, no server, deployed in minutes</li> <li>Ansible galaxy community to share modules.</li> <li>Define workflows to streamline jobs, run playbooks</li> <li>Integration of Webhook with GitLab and GitHub to automatically run automation or launch deployments based on source code pushes</li> <li>Scalable execution and increased runtime capacity as container on OCP</li> <li>Visual dashboard</li> <li>Role-based access control (RBAC)</li> <li>Job scheduling, integrated notifications</li> <li>Graphical inventory management</li> <li>Automation Analytics which enables users to see how specific automation actions perform in detail</li> </ul>"},{"location":"appsody/","title":"Appsody","text":"<p>Appsody provides pre-configured container images (stacks) and project templates for a growing set of popular open source runtimes and frameworks, providing a foundation on which to build applications for Kubernetes and Knative deployments. On k8s it uses the appsody operator to automate the installation and maintenance of a special type of Custom Resource Definitions (CRDs), called AppsodyApplication.</p> <p>On Mac install appsody with home brew:   <code>brew install appsody/appsody/appsody</code>. To update to new version: <code>brew upgrade appsody</code></p>"},{"location":"appsody/#how-it-works","title":"How it works","text":"<p>Appsody includes a CLI and a daemon to control the life cycle of the application. Developers use the CLI to create a new application from an existing \"stack\" (1).</p> <p></p> <p>Stacks are defined in a repository. Repositories can be referenced from remote sources (e.g., GitHub) or can be stored locally on a filesystem.</p> <p>First we can get the list of templates available via the command, which list stacks from all known repo:</p> <pre><code>appsody list\n</code></pre> <p>Then create our own application using one of the template:</p> <pre><code>mkdir projectname\nappsody init java-openliberty\n# another useful one\nappsody init quarkus\n# or ours\nappsody init ibmcase/ibm-gse-eda-quarkus\n</code></pre> <p>In general the command is <code>appsody init &lt;repository-name&gt;/&lt;stack&gt;</code>. It is possible to initialize an existing project using the command: <code>appsody init &lt;stackname&gt; --no-template</code>.</p> <p>Appsody helps developer to do not worry about the details of k8s deployment and build. During a Appsody run, debug or test step (2), Appsody creates a Docker container based on the parent stack Dockerfile, and combines application code with the source code in the template.</p> <p>When a source code project is initialized with Appsody, you get a local Appsody development container where you can do the following commands:</p> <pre><code>appsody run\nappsody test\nappsody debug\n</code></pre> <p>One of the above command creates a daemon which monitors changes to any files and build and start a new docker container continuously.</p> <pre><code># The daemon\nps -ef | grep appsody\n501 4156 93070 appsody run\n# the docker container\n501 56789 4156 docker run --rm -p 7777:7777 -p 9080:9080 -p 9443:9443 --name scoring-mp-dev -v /Users/jeromeboyer/.m2/repository:/mvn/repository -v /Users/jeromeboyer/Code/jbcodeforce/myEDA/refarch-reefer-ml/scoring-mp/src:/project/user-app/src -v /Users/jeromeboyer/Code/jbcodeforce/myEDA/refarch-reefer-ml/scoring-mp/pom.xml:/project/user-app/pom.xml -v appsody-controller-0.3.3:/.appsody -t --entrypoint /.appsody/appsody-controller docker.io/appsody/java-microprofile:0.2 --mode=run\n</code></pre> <p>The other basic commands are:</p> <ul> <li>Run, to run. But you can use docker options like:</li> </ul> <pre><code>appsody run --docker-options=\"--env-file=postgresql.properties\"\n# connect to a local docker network\nappsody run --network kafkanet\n</code></pre> <ul> <li>Build: You can use the <code>appsody build</code> command to generate a deployment Docker image on your local Docker registry, and then manually deploy that image to your runtime platform of choice.</li> <li>Deploy: You can use the <code>appsody deploy</code> command (3) to deploy the same deployment Docker image directly to a Kubernetes cluster that you are using for testing or staging. See next section. The full command template is:</li> </ul> <pre><code>appsody deploy -t &lt;mynamespace/myrepository[:tag]&gt; --push-url $IMAGE_REGISTRY --push --namespace mynamespace [--knative]\n# example\nappsody deploy -t jbsandbox/eda-coldchain-agent:0.0.1 --push-url $IMAGE_REGISTRY --push --namespace jbsandbox\n</code></pre> <ul> <li>To undeploy: <code>appsody deploy delete</code></li> <li>You can delegate the build and deployment steps to an external pipeline, such as a Tekton pipeline that consumes the source code of your Appsody project after you push it to a GitHub repository.</li> <li><code>appsody stop</code></li> </ul> <p>See Appsody CLI commands.</p> <p>See this tutorial how to deploy on openshift</p>"},{"location":"appsody/#app-deployment","title":"App Deployment","text":"<ol> <li>Log to the cluster</li> <li>Get the name of the available registry <code>oc get route --all-namespaces | grep registry</code>. Keep it in an env var: <code>export IMAGE_REGISTRY=default-route-openshift-image-registry.gse-eda-demo-202005-fa9ee67c9ab6a7791435450358e564cc-0000.us-south.containers.appdomain.cloud</code></li> <li>Login to this registry: <code>docker login -u $(oc whoami) -p $(oc whoami -t) $IMAGE_REGISTRY</code></li> <li>Create config map via yaml descriptor or command for all the dependent env variables, properties,... See notes in this repo</li> <li>Deploy using a command like:</li> </ol> <pre><code>appsody deploy -t jbsandbox/eda-coldchain-agent:0.0.1 --push-url $IMAGE_REGISTRY --push --namespace jbsandbox\n</code></pre> <p><code>appsody deploy -t dockerhub/imagename --push -n yournamespace</code> (3) will do the following:</p> <ul> <li>deploy Appsody operator into the given namespace if no operator found. (you can install it manually too see one of the instruction depending of the release)</li> <li>call <code>appsody build</code> and create a deployment image with the given tag</li> <li>push the image to docker hub or other repository</li> <li>create the <code>app-deploy.yaml</code> deployment manifest</li> <li>Apply it with:  <code>kubectl apply -f app-deploy.yaml</code>  within the given namespace</li> </ul> <p>Verify the operator is deployed:</p> <pre><code>oc get pods\nNAME                               READY     STATUS             RESTARTS   AGE\nappsody-operator-d8dfb4f5f-4dpwk   1/1       Running            0          4m34s\n</code></pre> <p>As part of the deployment manifest a service and a route are created. For example using a microprofile app the following command will verify everything went well.</p> <pre><code>curl http://scoring-mp-eda-sandbox.apps.green.ocp.csplab.local/health\n</code></pre> <pre><code>appsody deploy delete -n yournamespace\n</code></pre> <p>To ensure that the latest version of your app is pushed to the cluster, use the -t flag to add a unique tag every time you redeploy your app. Kubernetes then detects a change in the deployment manifest, and pushes your app to the cluster again.</p>"},{"location":"appsody/#appsody-app-creation-examples","title":"Appsody app creation examples","text":""},{"location":"appsody/#create-a-python-flask-app","title":"Create a python flask app","text":"<p>The stack is not for production and is not fully supported. Here is an example of creating a simple webapp with flask, flask cli and gunicorn</p> <pre><code># Get the default template from the stack\nappsody init incubator/python-flask\n# build an image with a name = the folder name based on the dockerfile from the stack\nappsody build\n# or run it directly\nappsody run\n</code></pre> <p>You can add your own dockerfile to extend existing one. With <code>docker images</code> you can see what <code>appsody build</code> created, then you can use this image as source for your own docker image</p> <pre><code>FROM &lt;nameofyourapp&gt;\nADD stuff\nCMD change the command\n</code></pre> <p>To add your own code.</p>"},{"location":"appsody/#create-quarkus-knative-app","title":"Create quarkus knative app","text":"<pre><code>appsody init quarkus\n</code></pre> <p>Note</p> <p>The container image will not be pushed to a remote container registry, and hence the container image url has to be <code>dev.local</code>, to make Knative deploy it without trying to pull it from external container registry.</p> <p>Then do the following steps:</p> <ul> <li>update the index.html to provide some simple doc of the app</li> <li>add health, and openapi</li> </ul> <p><code>xml       &lt;dependency&gt;       &lt;groupId&gt;io.quarkus&lt;/groupId&gt;       &lt;artifactId&gt;quarkus-smallrye-health&lt;/artifactId&gt;     &lt;/dependency&gt;     &lt;dependency&gt;       &lt;groupId&gt;io.quarkus&lt;/groupId&gt;       &lt;artifactId&gt;quarkus-smallrye-openapi&lt;/artifactId&gt;     &lt;/dependency&gt;</code> * If using kafka to support event driven solution add:</p> <pre><code>      &lt;dependency&gt;\n&lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n&lt;artifactId&gt;quarkus-smallrye-reactive-messaging-kafka&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"appsody/#create-a-microprofile-30-app","title":"Create a microprofile 3.0 app","text":"<pre><code>appsody init java-openliberty\n</code></pre>"},{"location":"appsody/#defining-your-own-stack","title":"Defining your own stack","text":"<p>See code in appsody-stacks/experimental/ibm-gse-eda-quarkus for one example of a kafka quarkus stack and Appsody tutorial to get detail instructions.</p> <p>Stack has one dockerfile to help building the application and control the build, run and test steps of <code>Appsody</code>. And a second Dockerfile in the <code>image/project</code> folder to \"dockerize\" the final app. This Dockerfile is responsible for ensuring the combined dependencies are installed in the final image. It hosts the target <code>app-deploy.yaml</code> file used for kubernetes deployment.</p> <p>When designing a stack, we need to decide who controls the application: a web server in which the developer, user of the stack, is adding new end points, or the developer is controlling how the app starts and runs.</p> <p>See details in this note.</p> <p>See appsody environment variables description in the product documentation</p> <p>See this other create appsody tutorial here.</p> <p>Some considerations to address:</p> <ul> <li>select the technologies and libraries to use</li> <li>address how to verify dependencies</li> <li>define what kind of sample starter application</li> <li>address how to enable adding new libraries</li> <li>define what docker image repository to use, and what credentials</li> </ul> <p>Here is a summary of the steps to create a Kafka java stack for consumer and producer:</p> <ul> <li>Look to local cache for appsody stack</li> </ul> <pre><code>$ export APPSODY_PULL_POLICY=IFNOTPRESENT\n</code></pre> <ul> <li>Create a starter stack, as a scaffold.</li> </ul> <p><pre><code>$ appsody stack create ibm-gse-eda-quarkus --copy incubator/java-openliberty\n</code></pre> * Update the template folder, and then the stack.yml to define version number (e.g. 1.7.1), name, ... * Update the pom.xml under <code>image</code> folder. * Under the experimental folder build the stack using the <code>Dockerfile-stack</code> file with the following command</p> <p><pre><code>$ appsody stack package --image-namespace ibmcase\n\nYour local stack is available as part of `dev.local` repo.\n</code></pre> This mean a file <code>ibm-gse-eda-quarkus.v1.7.1.templates.kafka.tar.gz</code> is created in <code>.appsody/stacks/dev.local/</code></p> <ul> <li>Test your stack scaffold</li> </ul> <pre><code>$ appsody init dev.local/ibm-gse-eda-quarkus kafka\n\n Successfully initialized Appsody project with the dev.local/gse-eda-java-stack stack and the kafka template.\n</code></pre> <ul> <li> <p>Start the application scaffold using <code>appsody run</code>. If you are running with a remote kafka broker set the <code>scripts/appsody.env</code> variables accordingly.</p> </li> <li> <p>Modify the <code>Dockerfile-stack</code> file to include the base image and dependencies for the server and other predefined code.</p> </li> <li> <p>Package your stack to create a docker images that will be pushed to dockerhub registry</p> </li> </ul> <pre><code>appsody stack package --image-namespace ibmcase --image-registry docker.io\n# this command builds a docker image but also creates files under ~/.appsody/stacks/dev.local\nibm-gse-eda-quarkus.v0.4.1.source.tar.gz\nibm-gse-eda-quarkus.v0.4.1.templates.default.tar.gz\nibm-gse-eda-quarkus.v0.4.1.templates.kafka.tar.gz\n# push the docker image created\ndocker push ibmcase/ibm-gse-eda-quarkus </code></pre> <ul> <li>If not done create a git release in the appsody-stack github repository. See the latest release</li> <li>Redefined the repository index, so from the source of all the stacks do</li> </ul> <p><pre><code>appsody stack add-to-repo ibmcase --release-url https://github.com/ibm-cloud-architecture/appsody-stacks/releases/download/0.4.1/\n# this command updates the following files\nibmcase-index.json\nibmcase-index.yaml\n</code></pre> * copy those file into root folder of the stack project</p> <ul> <li>Upload the source code and template archives to the release using drag and drop. The files are</li> </ul> <pre><code>ibm-gse-eda-quarkus.v0.4.1.source.tar.gz\nibm-gse-eda-quarkus.v0.4.1.templates.default.tar.gz\nibm-gse-eda-quarkus.v0.4.1.templates.kafka.tar.gz\nibmcase-index.json\nibmcase-index.yaml\n</code></pre> <p>then publish the release. Which can be see at the URL: https://github.com/ibm-cloud-architecture/appsody-stacks/releases.</p> <p><code>appsody repo add ibmcase https://raw.githubusercontent.com/ibm-cloud-architecture/appsody-stacks/master/ibmcase-index.yaml</code></p>"},{"location":"appsody/#future-readings","title":"Future readings","text":"<ul> <li>Introduction to Appsody: Developing containerized applications for the cloud just got easier</li> <li>Video Appsody overview</li> <li>Kabanero - Appsody - Tekton - Openshift</li> </ul>"},{"location":"code-ready/","title":"Code Ready","text":"<p>Red Hat tools to help developer get the most of k8s and get started quickly. It includes workspace, container, studio, builder, toolchain and dependencies.</p>"},{"location":"code-ready/#code-ready-container","title":"Code Ready Container","text":"<p>Red Hat CodeReady Containers brings a minimal OpenShift 4.0 or newer cluster to your local computer. See getting started here</p>"},{"location":"code-ready/#install-quick-summary","title":"Install quick summary","text":"<ul> <li>Download 1.8 G binary from here, extract it in a folder with $PATH. It shoud just create a crc command.</li> <li>Run <code>crc setup</code> to set up the environment of your host machine </li> <li>Start the VM <code>crc start</code>. Keep the password for the developer user.</li> <li><code>oc login -u developer -p developer https://api.crc.testing:6443</code> </li> <li>Stop the VM <code>crc stop</code></li> <li>Access the console <code>crc console</code> or <code>crc console --credentials</code></li> <li>Delete the VM: <code>crc delete</code></li> <li>The <code>crc ip</code> command can be used to obtain the VM IP address as needed</li> </ul> <p>Notes</p> <p>CodeReady Containers creates a /etc/resolver/testing file which instructs macOS to forward all DNS requests for the testing domain to the CodeReady Containers virtual machine.CodeReady Containers also adds an api.crc.testing entry to /etc/hosts pointing at the VM IP address. This is needed by the oc binary.</p> <p>To access the OpenShift web console, follow these steps:</p> <ol> <li>Run crc console. This will open your web browser and direct it to the web console.</li> <li>Log in to the OpenShift web console as the developer user with the password printed in the output of the crc start command or by running: <code>crc console --credentials</code></li> </ol> <p>To access to the administrator user login as kubeadmin, something like:</p> <p><code>oc login -u kubeadmin -p 7z6T5-qmTth-oxaoD-p3xQF https://api.crc.testing:6443</code></p>"},{"location":"code-ready/#use-oc-with-crc","title":"Use oc with CRC","text":"<p>To access the OpenShift cluster via the oc command:</p> <ul> <li><code>crc oc-env</code></li> <li>Get the Cluster Operators: <code>oc get co</code></li> </ul>"},{"location":"code-ready/#codeready-workspaces","title":"CodeReady workspaces","text":"<p>CodeReady is a web-based IDE running on Openshift and in a web browser, it is based on Eclipse Che 7.</p> <p>It is installed using the OperatorHub Catalog present in the OpenShift web console, see installation note</p> <p>It uses the concept of devfile to define the development environment as portable and committable to github.</p>"},{"location":"compendium/","title":"OpenShift Compendium","text":"<ul> <li>Getting Started with openshift on learn.openshift.com</li> <li>videos for Developers</li> <li>Red Hat demo system</li> <li>Openshift 4.2 learning playground</li> <li>Source-to-image (S2I)</li> <li>oc cluster wrapper</li> <li>REST API for openshift</li> <li>Minishift github</li> <li>Deploy openshift on IBM classic infrastructure</li> <li>openshift IKS</li> <li>Bastion node</li> <li> <p>Configure user and authorization in openshift</p> </li> <li> <p>Red hat Open TLC</p> </li> </ul>"},{"location":"compendium/#ibm-content-related-to-openshift","title":"IBM Content related to OpenShift","text":"<ul> <li>Red Hat OpenShift on IBM Cloud Develop in a preconfigured OpenShift environment available for four hours at no charge. With some hands on labs and introduction videos.</li> <li>Cloud Native Toolkit</li> </ul>"},{"location":"deployment-ex/","title":"Deployments examples","text":"<p>Multiple ways to deploy an app to openshift:</p> <ol> <li> <p>Deploy an application from an existing Docker image. (Using <code>Deploy Image</code> in the project view.)</p> <p></p> <p>Note</p> <p>There are two options: </p> <ul> <li>from an image imported in the openshift cluster, or built from a dockerfile inside the cluster. </li> <li>by accessing a remote image repository like <code>Dockerhub</code> or quay.io. The image will be pulled down and stored within the internal OpenShift image registry. The image will then be copied to any node in the OpenShift cluster where an instance of the application will be scheduled.</li> </ul> <p>Application will, by default, be visible internally to the OpenShift cluster, and usually only to other applications within the same project. Use <code>Create route</code> to make the app public. </p> </li> <li> <p>Build and deploy from source code contained in a Git repository using a Source-to-Image toolkit.</p> <p></p> <p>See this video to get s2i presentation and this section goes to a simple Flask app deployment using s2i. </p> </li> <li> <p>Build and deploy from source code contained in a Git repository from a Dockerfile.</p> </li> </ol> <p>Build the first time</p> <pre><code>oc new-build --binary --name=vertx-greeting-application -l app=vertx-greeting-application\nmvn dependency:copy-dependencies compile\noc start-build vertx-greeting-application --from-dir=. --follow\noc new-app vertx-greeting-application -l app=vertx-greeting-application\noc expose service vertx-greeting-application\n</code></pre> <p>To update the application, just update the code and run:</p> <pre><code>mvn dependency:copy-dependencies compile\noc start-build vertx-greeting-application --from-dir=. --follow\n</code></pre> <ol> <li> <p>Using Helm charts and helm CLI: Helm can be used as well to define the config files and deploy. Here is a new CI/CD example done from scratch based on the Reefer ML project simulator code.</p> <p>See getting started with helm guide.</p> <ul> <li>Create helm chart using the command <code>helm create</code></li> </ul> <pre><code>cd simulator/chart\nhelm create kcontainer-reefer-simulator\n</code></pre> <ul> <li> <p>Change the values.yaml to reflect environment and app settings. Remove Ingress as we will define Openshift route for the app to be visible.</p> </li> <li> <p>In the templates folder modify the deployment.yaml to add env variables section:</p> </li> </ul> <pre><code>env:\n- name: PORT\nvalue: \"{{ .Values.service.servicePort }}\"\n- name: APPLICATION_NAME\nvalue: \"{{ .Release.Name }}\"\n- name: KAFKA_BROKERS\nvalueFrom:\nconfigMapKeyRef:\nname: \"{{ .Values.kafka.brokersConfigMap }}\"\nkey: brokers\n- name: TELEMETRY_TOPIC\nvalue: \"{{ .Values.kafka.telemetryTopicName }}\"\n- name: CONTAINER_TOPIC\nvalue: \"{{ .Values.kafka.containerTopicName }}\"\n{{- if .Values.eventstreams.enabled }}\n- name: KAFKA_APIKEY\nvalueFrom:\nsecretKeyRef:\nname: \"{{ .Values.eventstreams.apikeyConfigMap }}\"\nkey: binding\n{{- end }}\n</code></pre> <ul> <li>Create helm template file for deployment:</li> </ul> <pre><code>helm template --output-dir templates --namespace eda-demo chart/kcontainer-reefer-simulator/\n</code></pre> <ul> <li>Push the service.yaml and deployment.yml template to the gitops repository under the branch <code>eda-demo/gse-eda-demos.us-east.containers.appdomain.cloud</code>.</li> <li>In the github repository define secrets environment variables for docker username and password, from your docker hub account.</li> <li>When pushing the repository the gitAction will perform the build.</li> </ul> </li> </ol>"},{"location":"deployment-ex/#deploy-zipkin-from-docker-image","title":"Deploy zipkin from docker image","text":"<p>Install it, and expose it with a service</p> <p><pre><code>oc new-app --docker-image=openzipkin/zipkin\n\noc expose svc/zipkin\n</code></pre> A new route is created visible with <code>oc get routes</code>. Once the hostname is added to a DNS or /etc/hosts. </p> <p>See zipkin architecture article here</p>"},{"location":"deployment-ex/#deploy-db2","title":"Deploy DB2","text":"<p>The Community edition DB2 image on dockerhub. It includes a predefined DB.</p> <p>Clone the DB2 repository to get the helm chart. See readme in this repo, for installation using helm but tiller needs to be installed before. The repository includes a script: <code>db2u-install</code></p> <p>Create your own docker image with a shell to create the schema:</p> <pre><code>FROM ibmcom/db2\n\nRUN mkdir /var/custom\nCOPY createschema.sh /var/custom\nRUN chmod a+x /var/custom/createschema.sh\n</code></pre> <p>Deinstalling configuration</p> <pre><code>oc delete -n jbsandbox sa/db2u role/db2u-role rolebinding/db2u-rolebinding\n</code></pre>"},{"location":"deployment-ex/#deploy-rabbitmq","title":"Deploy RabbitMQ","text":""},{"location":"deployment-ex/#first-install-operator","title":"First install operator","text":"<p>To create a RabbitMQ instance, a RabbitmqCluster resource definition must be created and applied. RabbitMQ Cluster Kubernetes Operator creates the necessary resources, such as Services and StatefulSet, in the same namespace in which the <code>RabbitmqCluster</code> CRD was defined.</p> <p>See some instructions here. Be sure to have kubectl &gt;= 1.14</p> <pre><code>git clone http://github.com/rabbitmq/cluster-operator.git\ncd cluster-operator\nkubectl create -f config/namespace/base/namespace.yaml\nkubectl create -f config/crd/bases/rabbitmq.com_rabbitmqclusters.yaml\n# Add cluster roles and roles: rabbitmq-cluster-operator-role and rabbitmq-cluster-leader-election-role\nkubectl -n rabbitmq-system create --kustomize config/rbac/\nkubectl -n rabbitmq-system create --kustomize config/manager/\n# Verify CRD installed\nkubectl get customresourcedefinitions.apiextensions.k8s.io | grep rabbit\n# Verify the service account\noc get sa rabbitmq-cluster-operator\n# link the service account to security policy\noc adm policy add-scc-to-user privileged rabbitmq-cluster-operator\n</code></pre> <p>If there is an error about user 1000 not in range, change the deployment yaml file for the value <code>securityContext.runAsUser: 1000570000</code> to a value in range and remove the other runAsGroup and fsGroup.</p>"},{"location":"deployment-ex/#create-a-cluster","title":"Create a cluster","text":"<p>when the operator pod runs, create one instance create a yaml file:</p> <pre><code>apiVersion: rabbitmq.com/v1beta1\nkind: RabbitmqCluster\nmetadata:\n  name: eda-rabbitmq\nspec:\n    replicas: 1\n</code></pre> <p>Then do</p> <pre><code>oc apply -f rabbit-cluster.yaml \noc get rabbitmqclusters\n</code></pre> <p>If an error like: \"services \\\"eda-rabbitmq-rabbitmq-headless\\\" is forbidden: cannot set blockOwnerDeletion if an ownerReference refers to a resource you can't set finalizers on\" happens, do the following: </p> <pre><code>\n</code></pre>"},{"location":"deployment-ex/#deploy-sparks","title":"Deploy sparks","text":"<p>Using the operator, see this note</p>"},{"location":"deployment-ex/#mongodb","title":"Mongodb","text":"<p>Define an env file with the following variables: MONGODB_USER=mongo MONGODB_PASSWORD=mongo MONGODB_DATABASE=reeferdb MONGODB_ADMIN_PASSWORD=password</p> <p>then run:</p> <pre><code>oc new-app --env-file=mongo.env --docker-image=openshift/mongodb-24-centos7\n</code></pre> <p>See the service:</p> <pre><code>oc describe svc mongodb-24-centos7\n</code></pre> <p>For more detail see this note</p>"},{"location":"deployment-ex/#deploy-jupyter-lab","title":"Deploy Jupyter lab","text":"<p>See this note to deploy Jupyter lab lastest image to Openshift using the Deploy Image choice. The deployment is done under the project <code>reefer-shipment-solution</code></p> <p></p> <p>The environment variable needs to be set to get Jupyter lab. </p> <p></p> <p>It takes multiple minutes to deploy. For the permission error due to the jovyan user not known, the command was:</p> <pre><code>oc adm policy add-scc-to-user anyuid -z default -n reefer-shipment-solution\n</code></pre> <pre><code> oc get routes\nNAME  HOST/PORT   PATH      SERVICES                PORT       TERMINATION   WILDCARD\njupyterlab              jupyterlab-reefer-shipment-solution.greencluster-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud    all-spark-notebook   8888-tcp      None\n</code></pre> <p>Get the secuity token to login in via the pod logs.</p> <pre><code>oc get pods \nNAME                             READY     STATUS      RESTARTS   AGE\nall-spark-notebook-2-z4dqx  \n</code></pre> <pre><code>oc logs all-spark-notebook-2-z4dqx \n</code></pre> <p>To avoid loosing the work on the notebook, we need to add PVC to <code>/home/jovyan/work</code> mount point</p> <pre><code>oc get dc\n\noc set volume dc/all-spark-notebook  --add --mount-path /home/jovyan/work --claim-size=1G\n</code></pre>"},{"location":"deployment-ex/#deploy-helm-tiller-deprecated","title":"Deploy helm / tiller (DEPRECATED)","text":"<p>The goal is to install Tiller server on its own project, and grant it permissions to one or more other projects where Helm Charts will be installed.</p> <p>See the instructions in this blog.</p> <p>Here is a quick summary of the commands performed</p> <pre><code>oc new-project tiller\noc project tiller\nexport TILLER_NAMESPACE=tiller\noc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE=\"${TILLER_NAMESPACE}\" -p HELM_VERSION=v2.16.4 | oc create -f -\n</code></pre> <p>Once deployed and Tiller server running, create a new project and grant tiller edit role to access this new project, and then use helm CLI to deploy the app:</p> <pre><code>oc new-project myapp\noc policy add-role-to-user edit \"system:serviceaccount:${TILLER_NAMESPACE}:tiller\"\n</code></pre>"},{"location":"os-cn-dev/","title":"Cloud Native Development Summary","text":""},{"location":"os-cn-dev/#source-to-image-s2i","title":"Source to image (s2i)","text":"<p>Source to image toolkit aims to simplify the deployment to OpenShift. It uses a build image to execute an assembly script that builds code and docker image without Dockerfile.  </p> <p>The following figure, shows the resources created by the <code>oc new-app</code> command when the argument is an application source code repository.</p> <p></p> <p>From an existing repository, <code>s2i create</code> add a set of elements to define the workflow into the repo. For example the command below will add Dockerfile and scripts to create a build image named <code>ibmcase/buildorderproducer</code> from the local folder where the code is.</p> <pre><code>s2i create ibmcase/buildorderproducer .\n</code></pre> <p>When the assemble script is done, the container image is committed to internal image repository. The CMD part of the dockerfile execute a run script.</p> <p>Here is another command to build the output image using existing build image on local code:</p> <pre><code>s2i build --copy .  centos/python-36-centos7 ibmcase/orderproducer\n</code></pre> <p>Note</p> <p>s2i takes the code from git, so to use the local code before committing it to github, add the <code>--copy</code> argument.</p> <ul> <li>OpenShift builds applications against an image stream. The OpenShift installer populates several image streams by default during installation.  </li> </ul> <pre><code> oc get is -n OpenShift\n</code></pre> <p>If only a source repository is specified, oc new-app tries to identify the correct image stream to use for building the application</p>"},{"location":"os-cn-dev/#odo-openshift-do","title":"ODO: Openshift Do","text":"<p>ODO is a CLI for developer to abstract kubernetes. It can build and deploy your code to your cluster immediately after you save your changes. <code>odo</code> helps manage the components in a grouping to support the application features. A selection of runtimes, frameworks, and other components are available on an OpenShift cluster for building your applications. This list is referred to as the Developer Catalog.</p> <p>The main value propositions are:</p> <ul> <li>Abstracts away complex Kubernetes and OpenShift commands and configurations.</li> <li>Detects changes to local code and deploys it to the cluster automatically, giving instant feedback to validate changes in real time</li> </ul> <p>V2.0 is merging with Appsody where stacks are devfile in odo. A devfile is a portable file that describes your development environment. See some devfile examples</p>"},{"location":"os-cn-dev/#important-concepts","title":"Important concepts","text":"<ul> <li>Init containers are specialized containers that run before the application container starts and configure the necessary environment for the application containers to run. Init containers can have files that application images do not have, for example setup scripts</li> <li>Application container is the main container inside of which the user-source code executes. It uses two volumes: emptyDir and PersistentVolume. The data on the PersistentVolume persists across Pod restarts.</li> <li>odo creates a Service for every application Pod to make it accessible for communication</li> <li>odo push workflow: <ul> <li>create resources like deployment config, service, secrets, PVC</li> <li>index source code files </li> <li>push code into the application container</li> <li>execute assemble and restart.</li> </ul> </li> </ul>"},{"location":"os-cn-dev/#installing-odo","title":"Installing odo","text":"<p>Last instructions to install. </p> <pre><code># For MAC\ncurl -L https://mirror.openshift.com/pub/openshift-v4/clients/odo/latest/odo-darwin-amd64 -o /usr/local/bin/odo\nchmod +x /usr/local/bin/odo\n# For VSCode: Command P and\next install redhat.vscode-openshift-connector\n</code></pre> <p>List existing software runtime catalog deployed on a cluster (For example Java and nodejs are supported runtimes):</p> <pre><code>odo catalog list components\n</code></pre>"},{"location":"os-cn-dev/#developing-with-odo","title":"Developing with ODO","text":"<pre><code># login to a OpenShift cluster like ROKS\nodo  login --token=s....vA c100-e.us-south.containers.cloud.ibm.com:30040\n# Create a new project in OCP\nodo project create jbsandbox\n# change project inside OCP\nodo project set jbsandbox # Create a component (from an existing project for ex)... then follow the different questions\nodo component create\n# list available component, with v2, the devfile list is also returned\nodo catalog list components\n# push to ocp\nodo push\n# delete an app\nodo app list\nodo app delete myapp\n</code></pre> <p>Deploy a component to OpenShift creates 2 pods: one ...app-deploy and one ...-app</p> <p>To create a component (create a config.yml) from a java springboot app, once the jar is built, the following command defines (a <code>backend</code> named component) to run it on top of the java runtime:</p> <p><pre><code>odo create java backend --binary target/wildwest-1.0.jar\n</code></pre> The component is not yet deployed on OpenShift. With an odo create command, a configuration file called config.yaml has been created in the local directory. To see the config use:</p> <pre><code>odo config view\n\nCOMPONENT SETTINGS\n------------------------------------------------\nPARAMETER         CURRENT_VALUE\nType              java:8\nApplication       app\nProject           myproject\nSourceType        binary\nRef\nSourceLocation    target/wildwest-1.0.jar\nPorts             8080/TCP,8443/TCP,8778/TCP\nName              backend\n</code></pre> <p>Then to deploy the binary jar file to Openshift:</p> <p><pre><code>odo push\n</code></pre> OpenShift has created a container to host the backend component, deployed the container into a pod running on the OpenShift cluster, and started up the backend component. You can view the backend component being started up, in the <code>Developer</code> perspective, under the <code>Topology</code> view. When a dark blue circle appears around the backend component, the pod is ready and the backend component container will start running on it. (A light blue ring means the pod is in a pending state and hasn't started yet)</p> <p>OpenShift provides mechanisms to publish communication bindings from a program to its clients. This is referred to as linking. To link the current frontend component to the backend: </p> <pre><code>odo link backend --component frontend --port 8080\n</code></pre> <p>This will inject configuration information into the frontend about the backend and then restart the frontend component.</p> <p>To expose an application to external client, we need to add a URL:</p> <pre><code>odo url create frontend --port 8080\nodo push\n</code></pre> <p>To adapt to the developer changes, we can tell odo to watch for changes on the file system in the background using:</p> <pre><code>odo watch\n</code></pre> <p>Once the change is recognized, odo will push the changes to the frontend component and print its status to the terminal. </p>"},{"location":"os-cn-dev/#see-odo-github","title":"See odo github","text":""},{"location":"os-cn-dev/#python-flask","title":"Python Flask","text":"<p>Current base code is a Flask under https://github.com/odo-devfiles/python-ex</p> <pre><code>mkdir project-name\n# create the devfile and download starter code\nodo create python --starter\n# Deploy to OCP\nodo push\n</code></pre>"},{"location":"spark-on-os/","title":"Kubernetes Operators","text":""},{"location":"spark-on-os/#operators","title":"Operators","text":"<p>Operators make it easy to manage complex stateful applications on top of Kubernetes. They are custom resource definition in kubernetes. See the framework here.</p> <p>Operator SDK is a framework to expose higher level APIs to write operational logic.</p> <p>Here is a workflow for a new Go-based Operator using the Operator SDK:</p> <ul> <li> <p>Create a new Operator project using the SDK CLI.</p> <pre><code>operator-sdk new podset-operator --type=go --skip-git-init\n</code></pre> </li> <li> <p>Create a new Custom Resource Definition API Type using the SDK CLI.</p> <pre><code>operator-sdk add api --api-version=app.example.com/v1alpha1 --kind=PodSet\n</code></pre> </li> <li> <p>Add your Custom Resource Definition (CRD) to your live Kubernetes cluster.</p> </li> <li>Define your Custom Resource Spec and Status.</li> <li>Create a new Controller for your Custom Resource Definition API.</li> <li>Write the reconciling logic for your Controller.</li> <li>Run the Operator locally to test your code against your live Kubernetes cluster.</li> <li>Add your Custom Resource (CR) to your live Kubernetes cluster and watch your Operator in action!</li> <li>After you are satisifed with your work, use the SDK CLI to build and generate the Operator Deployment manifests.</li> <li>Optionally add additional APIs and Controllers using the SDK CLI.</li> </ul> <p>A namespace-scoped operator (the default) watches and manages resources in a single namespace, whereas a cluster-scoped operator watches and manages resources cluster-wide.</p> <p>Note<p>TBC</p> </p>"},{"location":"spark-on-os/#spark-on-openshift-using-operator","title":"Spark on openshift using operator","text":"<p>The following spark operator from : <code>git clone https://github.com/radanalyticsio/spark-operator.git</code> supports both config map or custom resource definition to deploy spark cluster on kubernetes. It works well in environments where a user has a limited role-based access to Kubernetes, such as OpenShift.</p> <p>See also this note on user identity. </p> <p>You need a custom spark docker image to set userid and pod security policies. </p>"},{"location":"spark-on-os/#spark-operator","title":"Spark operator","text":"<p>To deploy the spark operator, use one of the yaml file (with CRD or with configMap)</p> <pre><code>kubectl apply -f manifest/operator.yaml \n\nor\n\nkubectl apply -f manifest/operator-cm.yaml </code></pre>"},{"location":"spark-on-os/#spark-cluster","title":"Spark cluster","text":"<p>Once the operator is deployed, configure your spark cluster using one of the yaml examples</p> <pre><code>cd examples\nkubectl apply -f cluster.yaml \nor\n\nkubectl apply -f cluster-cm.yaml \n</code></pre> <p>Once you don't need the cluster anymore, you can delete it by deleting the custom resource by: <pre><code>kubectl delete sparkcluster my-spark-cluster\n</code></pre></p>"},{"location":"spark-on-os/#exposing-the-spark-ui","title":"Exposing the spark UI","text":"<p>Create a route for the Spark UI:</p> <ul> <li>go to the service to get the exposed port for the my-spark-cluster-ui service. </li> <li>In network&gt; routes add the roule with the following yaml:</li> </ul> <pre><code>apiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\nname: expose-spark-ui\nnamespace: greencompute\nspec:\npath: /\nto:\nkind: Service\nname: my-spark-cluster-ui\nport:\ntargetPort: 8080\n</code></pre> <p>Then going to the exposed URL will bring the basic spark user interface, helpful to get visibility on the running and completed application, and the workers state.</p> <p></p>"},{"location":"spark-on-os/#test-with-spark-terminal-and-scala","title":"Test with spark terminal and scala","text":"<p>Go to one of the spark worker pod, in the Workloads menu, then go to the Terminal and enter:</p> <pre><code>spark-shell\n</code></pre> <p>you should now be in scala interpretor. Spark\u2019s shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively.</p> <p></p> <p>The simple exercise will be to count the number of word occurence in a file. First connect to one of the spark pod using kubectl or oc.</p> <pre><code>oc get pods\nNAME                             READY     STATUS    RESTARTS   AGE\nmy-spark-cluster-m-4gmdb         1/1       Running   0          1d\nmy-spark-cluster-w-jh9pl         1/1       Running   1          1d\nmy-spark-cluster-w-mnvkz         1/1       Running   0          1d\n\noc exec -ti my-spark-cluster-m-4gmdb bash\n\nbash-4.2$ cd /tmp; vi input.txt\n\nenter some sentences\n</code></pre> <p>In the spark-shell terminal, enter the following scala line of code to connect to spark context (sc) variable, read the file </p> <pre><code>scala&gt; val inputfile = sc.textFile(\"input.txt\")\ninputfile: org.apache.spark.rdd.RDD[String] = input.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:24\nscala&gt; val counts = inputfile. flatMap (line =&gt; line. split (\" \")).map (word =&gt; (word, 1)).reduceByKey (_+_)\ncounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[6] at reduceByKey at &lt;console&gt;:25\nscala&gt; counts.saveAsTextFile (\"output\")\n</code></pre> <p>The output is a folder under the /tmp directory. Inside the cluster executor container, we can see the file <code>cat part-00000</code> that contains the word count.</p> <p>Spark works!.</p> <p>See my other repo on spark studies</p> <p>The most common way to launch spark applications on the cluster is to use the shell command spark-submit. </p>"},{"location":"sre/","title":"System Reliability Engineering","text":"<p>Some potential failures to consider for each deployment:</p>"},{"location":"sre/#availability","title":"Availability","text":"<ul> <li>Always ask what are the SLAs</li> <li>Queue full and transaction not being processed</li> <li>API backend not available</li> <li>Network failure</li> <li>DNS problems</li> <li>Update for new version fails to deploy</li> <li>Hitting unknown limits, like number of VPC interfaces</li> <li>Certificate not auto renewed on expiry, and authentication fails</li> </ul>"},{"location":"sre/#performance","title":"Performance","text":"<ul> <li>Cold start of the function cause deploy</li> <li>Unreliable response time</li> <li>Consumer lag behind</li> <li>Event backbone not responding </li> <li>Functions lock an external resource which becomes a bottleneck</li> <li>Backend DB starts to throttle at high load</li> <li>Serverless stars to throttle at high volume</li> <li>Front end times out, no response from the back end</li> <li>Latency measurement</li> <li>Meaningful throughput measurement</li> <li>Batch job impacting main real time processing</li> <li>No more in SLA</li> <li>Retry logic missing jitter causes bursts of retries</li> <li>Message Queues are filled too quickly, causing unhandled messages</li> <li>Network latency impacting call / response - and front end doesn't handle</li> <li>Can't scale up fast enough to support demand spikes</li> <li>Transaction chain scales at different rates or hard limits on one link break scalability</li> </ul>"},{"location":"sre/#data-loss-durability","title":"Data Loss / Durability","text":"<ul> <li>Data at rest gets corrupted</li> <li>Error while archiving central logs</li> <li>Every nth transaction fails silently, loses data</li> <li>DR site down</li> <li>Data out of synch between two data sources</li> <li>Can't find a particular orderID in the logs</li> <li>Lack of log files in serverless world</li> <li>Data replicated among active cluster is taking longer than expected</li> <li>Queue exceeds visibility limit</li> <li>Losing messages in kafka</li> <li>Duplicated messages</li> </ul>"},{"location":"sre/#quality-and-correctness","title":"Quality and correctness","text":"<ul> <li>Idempotency failure causing multiple transactions in error</li> <li>Dual records created in database</li> <li>See unexpected records</li> <li>Version error after code upgrade</li> <li>Data recovery can cause ID duplicates</li> <li>Error reporting not reporting actual problem</li> <li>Are the runbooks autodated?</li> <li>Does simulated transaction bring clear understanding of the process flow and error flow?</li> <li>Inconsistent view between caller and callee? </li> </ul>"},{"location":"sre/#security","title":"Security","text":"<ul> <li>Privacy laws enforcement</li> <li>GDPR</li> <li>How to identify data breach</li> <li>Data at rest not encrypted and protected</li> <li>Elevation of privileges on PROD accounts</li> <li>Access to private data</li> <li>Cannot trace who make changes</li> <li>Keep up to date with security patches</li> <li>Open source component with security vulnerability goes undetected</li> </ul>"},{"location":"k8s/k8s-0/","title":"Kubernetes Introduction","text":"<p>Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.  It groups containers that make up an application into logical units for easy management and discovery.</p>"},{"location":"k8s/k8s-0/#key-features","title":"Key features","text":"<p>A Kubernetes cluster consists of one or more physical or virtual machines, also known as worker nodes, that are loosely coupled, extensible, and centrally monitored and managed by the Kubernetes master nodes.</p> <p>A cluster defines a set of resources, nodes, networks, and storage devices that keep applications highly available.</p> <ul> <li>Service discovery by assigning a single DNS entry to each set of containers, service discovery permits load-balancing the request across the pool of containers providing the service, and supports automatic load balancing around failure.</li> <li>Horizontal scaling is supported by adding new worker nodes to the cluster, or by adding more service instances</li> <li>Self-healing uses user-defined health checks to monitor containers liveness and restart and reschedule them in case of failure.</li> <li>Automated rollout gradually roll updates out to your application's containers while checking their status. If something goes wrong during the rollout, Kubernetes can roll back to the previous iteration of the deployment.</li> <li>Secrets and configuration management: isolate confidential data and configuration as standalone entities</li> <li>Operators packaged Kubernetes applications that also bring the knowledge of the application's life cycle into the Kubernetes cluster.  Applications packaged as Operators use the Kubernetes API to update the cluster's state reacting to changes in the application state.</li> </ul>"},{"location":"k8s/k8s-0/#components","title":"Components","text":"<p>The following diagram lists the important components of the cluster.</p> <p></p> <ul> <li>Master nodes control nodes, and schedule pods. They persist states and configuration in etcd.</li> <li>API server exposes API for the CLI and Web App to validate and support configuration injection</li> <li>kube-control manager is a daemon that embeds other controllers: node, replications, endpoints, and service account controllers.</li> <li>etcd: is a distributed key-value pair datastore to persist configuration, to do service discovery and to coordinate distributed work. Backup it.</li> <li>kube-proxy, is present in each node and perform TCP/UDP packet forwarding across the backend network. It is a network proxy for the services defined in the cluster.</li> <li>kubelet is the node agent to register a node to the cluster masters. It ensures pods in its node are healthy.</li> </ul> <p>Container images confine the application code, its runtime, and all of its dependencies in a pre-defined format.  Container runtime uses those pre-packaged images, to create one or more containers. They run in one host.  To have a fault-tolerant and scalable solution we need multiple nodes connected together and controlled by a container orchestrator.  It ensures that applications:</p> <ul> <li>are fault-tolerant</li> <li>can do horizontal scaling, and do this on-demand. Scale applications based on resource usage like CPU and memory.</li> <li>support automatic binpacking: schedules the containers based on resource usage and constraints, without sacrifying the availability</li> <li>are self-healed: automatically replaces and reschedules the containers from failed node</li> <li>can discover other applications automatically, and communicate with each other</li> <li>groups sets of containers and refers to them via a DNS name (called a service). It can discover these services automatically, and load-balance requests between containers of a given service</li> <li>are accessible from the external world</li> <li>can update/rollback, without any downtime, new versions/configurations of an application</li> <li>access storage orchestrated via Software Defined Storage</li> <li>support batch execution</li> <li>support VMs, bare-metal, or public/private/hybrid/multi-cloud setups</li> </ul>"},{"location":"k8s/k8s-0/#value-propositions","title":"Value Propositions","text":"<p>The key paradigm of Kubernetes is its <code>Declarative model</code>: you provide the \"desired state\" and Kubernetes will do it's best to make it happens.</p> <ul> <li>high availability 24/7</li> <li>deploy new version multiple times a day</li> <li>containerization of apps and business services</li> <li>helps you make sure those containerized applications run where and when you want, and helps them find the resources and tools they need to work</li> <li>Single-tenant Kubernetes clusters with compute, network and storage infrastructure isolation</li> <li>Automatic scaling of apps</li> <li>Use the cluster dashboard to quickly see and manage the health of your cluster, worker nodes, and container deployments.</li> <li>Automatic re-creation of containers in case of failures</li> <li>Polyglot applications</li> </ul>"},{"location":"k8s/k8s-0/#value-propositions-for-container","title":"Value propositions for container","text":"<p>Just to recall the value of using container for the cloud native application are the following:</p> <ul> <li>Docker ensures consistent environments from development to production. Docker containers are configured to maintain all configurations and dependencies internally.</li> <li>Docker containers allows you to commit changes to your Docker image and version control them. It is very easy to rollback to a previous version of your Docker image. This whole process can be tested in a few minutes.</li> <li>Docker is fast, allowing you to quickly make replications and achieve redundancy.</li> <li>Isolation: Docker makes sure each container has its own resources that are isolated from other containers</li> <li>Removing an app/ container is easy and won\u2019t leave any temporary or configuration files on your host OS.</li> <li>Docker ensures that applications that are running on containers are completely segregated and isolated from each other, granting you complete control over traffic flow and management</li> </ul> <p>The container filesystem is represented as a list of read-only layers stacked on top of each other using a storage driver.  The layers are generated when commands are executed during the Docker image build process. The top layer has read-write permissions. </p> <p>Docker daemon configuration is managed by the Docker configuration file (/etc/docker/daemon.json) and Docker daemon startup  options are usually controlled by the systemd unit: <code>docker</code>. With environment variables you can control one container, while using <code>linked containers</code> docker automatically copies  all environment variables from one container to another.</p>"},{"location":"k8s/k8s-0/#concepts","title":"Concepts","text":"<p>Kubernetes groups containers that make up an application into logical units for easy management and discovery.</p> <p></p> <p>Every containerized app that is deployed into a Kubernetes cluster is deployed, run, and managed by a pod. An app might require a container and other helper containers to be deployed into one pod, so that those containers can be addressed by using the same private IP address</p> <p>When a container image runs, it is also executed using namespaces in the operating system. These namespaces contain  the process and provide isolation: running container has its own separated file system, own network, own process identifier  namespace (PID). While Control Group (CGROUP) allows isolation of hardware resource.</p> <ul> <li>The Container Runtime offloads the IP assignment to CNI Container Network Interface: https://github.com/containernetworking/cni. CNI is a specification to define how network interfaces are set for container runtime.</li> <li>In cluster, pod to pod communication should happen across nodes, without any Network Address Translation. So k8s uses Software Define Networking like Calico to support networking https://www.projectcalico.org/</li> </ul>"},{"location":"k8s/k8s-0/#kubernetes-objects","title":"Kubernetes Objects","text":"<p>With each object, you declare the intent or desired state using the spec attribute. To create an object, we need to  provide the spec field to the Kubernetes API Server. The spec field describes the desired state, along with some basic  information, like the name, image name and version.</p> <ul> <li>Pod: A Pod is a logical collection of one or more containers</li> <li>Labels: are key-value pairs that can be attached to any Kubernetes objects. Labels are used to organize and select a subset of objects.</li> <li> <p>Label Selectors, we can select a subset of objects. Two types:</p> <ul> <li>Equality-Based Selectors allow filtering of objects based on label keys and values</li> <li>Set-Based Selectors allow filtering of objects based on a set of values (<code>in, notin, and exist</code>  operators)</li> </ul> </li> <li> <p>A ReplicationController (rc) is a controller that is part of the Master Node's Controller Manager. It makes sure  the specified number of replicas for a Pod is running at any given point in time.</p> </li> <li>A ReplicaSet (rs) is the next-generation ReplicationController. ReplicaSets support both equality- and set-based Selectors,  whereas ReplicationControllers only support equality-based Selectors. A Deployment automatically creates the ReplicaSets.</li> <li>Deployment objects provide declarative updates to Pods and ReplicaSets. The DeploymentController is part of  the Master Node's Controller Manager, and it makes sure that the current state always matches the desired state.  Deployments include the definitions for the app to run, it references the docker image to use and which port number exposed  to access the app. When you create a deployment, a Kubernetes pod is created for each container that you defined in the  deployment. To make your app more resilient, you can define multiple instances of the same app in your deployment and let  Kubernetes automatically create a Replica set for you.</li> </ul> <p>When a version of the container image change it is possible to deploy it, and it will create a new replication set.  This process is referred to as a Deployment rollout. Once ReplicaSet B is ready, the Deployment starts pointing to it.  On top of ReplicaSets, Deployments provide features like Deployment recording, with which, if something goes wrong,  we can rollback to a previously known state.</p> <ul> <li>namespace: If we have numerous users whom we would like to organize into teams/projects, we can partition  the Kubernetes cluster into sub-clusters using Namespaces. The names of the resources/objects created inside a Namespace are unique, but not across Namespaces. Generally, Kubernetes creates two default namespaces: kube-system and default.  Using Resource Quotas, we can divide the cluster resources within Namespaces. OpenShift maps project to namespace.</li> <li>Services: allow containers in one pod to open network connections to containers in another pod.  In the declaration, the targetPort attribute has to match a containerPort from a pod container definition,  and the port attribute is the port that is exposed by the service. selector is how the service finds pods  to forward packets to. Each service is dynamically assigned an SRV record with an FQDN of the form:  <code>SVC_NAME.PROJECT_NAME.svc.cluster.local</code></li> </ul>"},{"location":"k8s/k8s-0/#volumes","title":"Volumes","text":"<p>All data stored inside a container is deleted if the container crashes.  A Volume is essentially a directory  backed by a storage medium. The storage medium and its content are determined by the Volume Type. A Volume is attached  to a Pod and shared among the containers of that Pod. The Volume has the same life span as the Pod, and it outlives  the containers of the Pod - this allows data to be preserved across container restarts.</p> <ul> <li>A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator.  It is a resource in the cluster</li> <li>A PersistentVolumeClaim (PVC) is a request for storage by a user/app. It is similar to a pod. Pods consume node  resources and PVCs consume PV resources.</li> <li> <p>A StorageClass provides a way for administrators to describe the \u201cclasses\u201d of storage they offer.  Different classes might map to different quality-of-service levels, and to backup policies. Each StorageClass contains  the fields provisioner, parameters, and reclaimPolicy. vSphere, minio, GlusterFS, NFS are storage class.</p> </li> <li> <p>Platform Storage: uses hostPath storage. Consider to make those paths separate, expandable disks so they can be extended  as needed.</p> </li> <li>Block Storage: PV is a block of storage.  </li> </ul> <p>There are two ways PVs may be provisioned: statically or dynamically.</p> <ul> <li>Static: A cluster administrator creates a number of PVs. They carry the details of the real storage which is available  for use by cluster users. Cluster administrators must create their disks and export their NFS shares in order for Kubernetes  to mount them. Admin defines a PersistentVolume yaml file.</li> <li>Dynamic: When none of the static PVs the administrator created matches a user\u2019s PersistentVolumeClaim, the cluster may  try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a class and the administrator must have created and configured that class in order for dynamic provisioning to occur.</li> </ul> <p>A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a pod.</p> <ul> <li>Master nodes need to run on fast disks because they do use a lot of IO because of ETCD. Use SSDs. </li> <li>Do not run with thin provisioning for VM</li> <li>Separate SAN per cluster. </li> <li>When the cluster runs for a long time, the amount data persisted will become bigger</li> <li>Do not use hostPath storage for user's workload</li> <li>NFS shared storage is used for the docker trusted registry imave repository, but when using a load balancer in front  of multiple master nodes, NFS may become a problem due to simultaneous pushes to different nodes of the images.  If NFS sync does not happen fast enough you can get 'unknown blob' errors.</li> <li>NFS shared storage can add significant load to the data network at times of high usage.  Openshift deprecated NFS for the cluster.</li> <li>With SAN the bottle neck is the sas controller.</li> <li>KVM: kernel based VM is a linux feature to do virutalization. It is faster than VM.</li> <li>NVMe non-volatile memory express. Do not assume IOPS because of the hardware used, verify configuration. </li> <li>The tool to test io is fio </li> </ul> <p>Read more from k8s doc</p>"},{"location":"k8s/k8s-0/#configmaps","title":"ConfigMaps","text":"<p>ConfigMaps allow us to decouple the configuration details from the container image, it passes configuration parameters into the runtime pods without creating different docker images.</p> <ul> <li>Create a ConfigMap with command:</li> </ul> <p><pre><code>kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2\n</code></pre> or from a yaml file in which we mentioned the kind, metadata, and data fields, which are targeted to connect with  the v1 endpoint of the API Server. </p> <p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: vaccine-order-cm\ndata:\ntext_filed: \"Hello\"\nvoro_url: \"voro.vaccine.svc.cluster.local\"\n</code></pre> * In deployment descriptor all the environment variables will visible via  declaration like:</p> <pre><code>spec:\ntemplate:\nspec:\ncontainers:\nname: demoapp\nenvFrom:\n- configMapRef:\nname: vaccine-order-cm\n</code></pre> <ul> <li>Create a config map from a properties file</li> </ul> <pre><code>oc create configmap  demo-app-cm --from-file=./kafka.properties\n</code></pre> <p>Then in deployment mount it to the target path:</p> <pre><code>    spec:\ncontainers:\n- name: es-demo\nvolumeMounts:\n- mountPath: /deployments\nname: properties\nreadOnly: true\nsubPath: \"\"\nvolumes:\n- name: properties\nsecret:\nsecretName: demo-app-cm\n</code></pre>"},{"location":"k8s/k8s-0/#secret","title":"Secret","text":"<p>With Secrets, we can share sensitive information like passwords, tokens, or keys in the form of key-value pairs, encrypted and safe. In Deployments or other system components, the Secret object is referenced, without exposing its content. </p> <pre><code>kubectl create secret generic my-password --from-file=password.txt\n</code></pre> <p>Secret values need to be encrypted with base64. As an example a DB url could be encrypted using: </p> <p><pre><code>echo \"jdbc:db2://dashdb--.....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\" | base64\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: vaccine-order-db-secret\ndata:\ndb.url: amRiYzpkY...\n</code></pre></p> <p>We can get Secrets to be used by containers in a Pod by mounting them as data volumes. </p> <pre><code>- name: KAFKA_CERT_PWD\nvalueFrom:\nsecretKeyRef:\nkey: ca.password\nname: kafka-cluster-ca-cert\n</code></pre> <p>&gt;&gt; NEXT</p>"},{"location":"k8s/k8s-1/","title":"Networking","text":"<p>The Cloud Native Computing Foundation (CNCF) sponsors the Container Networking Interface (CNI) open source project which  aims to standardize the network interface for containers in cloud native environments, such as Kubernetes.</p> <p>Docker uses the CNI project to implement a software-defined network (SDN) for containers on each host.</p>"},{"location":"k8s/k8s-1/#docker-networking","title":"Docker networking","text":"<p>With Docker there are different networking modes: <code>bridge, host, container or no networking</code>. With <code>bridge</code> mode docker daemon creates a <code>docker0</code> virtual ethernet bridge to forward packets to any interface attached to it. All containers running on the host are attached to this bridge network. Peer attached to the container <code>eth0</code> interface got  an IP address and are part of the same subnet. The default\u00a0bridge\u00a0network is present on all Docker hosts.   If you do not specify a different network, new containers are automatically connected to the default\u00a0bridge\u00a0network. The following diagram illustrates those concept for two containers running on your laptop as a host:</p> <p> </p> <p>The following commands are used to understand the diagrams:</p> <pre><code># Create a network\ndocker network create --driver bridge alpine-net\ndocker run -dit --name alpine1 --network alpine-net -p 8081:8080 app1 ash\ndocker run -dit --name alpine2 --network alpine-net -p 8082:8080 app2 ash\ndocker network list\ndocker network inspect alpine-net\n# in one of the container\nip addr show\n</code></pre> <p>Containers are isolated from the host network. Docker containers can talk to other containers only if they are on the same  machine. Because of network isolation, a container in one SDN can have the same IP address as a container in a different SDN.</p> <p>With <code>Host</code> mode  (<code>docker run -d --net=host  yourimagehere</code>) the containers share IP and namespace, and you need to take  care of the port numbering schema. There is no routing overhead, but this approach exposes the container to the public network.  </p> <p>Docker does not support automatic service discovery on the default bridge network. If you want containers to be able to resolve IP addresses by container name, you should use\u00a0user-defined networks\u00a0instead.</p> <p>Exposing ports in a Dockerfile is a way of documenting which ports are used, but\u00a0does not actually map or open any ports.  When starting the container the exposed port is published to a port accessible by others. So for the previous figure, we have:</p> <pre><code>$ docker run \u2013d  -p 8081:8080 imagename\n</code></pre>"},{"location":"k8s/k8s-1/#kubernetes-networking","title":"Kubernetes networking","text":"<p>Kubernetes uses the <code>container mode</code>, where other containers share the same IP address as a previously created containers within the pod: it is set at the pod scope level. It requires each pod to have an IP in a flat networking namespace with full connectivity to other nodes and pods across the network.</p> <p>The Pod IP address is allocated by Calico IPAM and the networking interface definition within the pod and host is done by Calico CNI plugin.  The following diagram illustrates the process:</p> <p></p> <p>The following commands help to see these IP schema:</p> <pre><code>$ kubectl get node -o wide\nNAME           STATUS  VERSION          EXTERNAL-IP  OS-IMAGE             KERNEL-VERSION      \n172.16.40.137  Ready   v1.10.0+icp-ee   &lt;none&gt;       Ubuntu 16.04.3 LTS   4.4.0-112-generic   \n172.16.40.139  Ready   v1.10.0+icp-ee   &lt;none&gt;       Ubuntu 16.04.3 LTS   4.4.0-112-generic  \n# The 172.16.40.137 is the IP address of the HOST\n$ kubectl describe node 172.16.40.137\n# but ssh to the host/node and looking at the interface, we can see the ethernet and docker interfaces:\n$ ifconfig\nens160    Link encap:Ethernet  HWaddr 00:50:56:a5:cf:34  \n          inet addr:172.16.40.137  Bcast:172.16.255.255  Mask:255.255.0.0\n\ndocker0   Link encap:Ethernet  HWaddr 02:42:73:1c:72:ca  \n           inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:255.255.0.0\n</code></pre> <p>This IP-per-pod model yields a backward-compatible way for you to treat a pod almost identically to a VM or a physical host,  in the context of naming, service discovery, or port allocations. Containers inside a pod can communicate with each other   using <code>localhost</code>.  </p> <p>Kubernetes uses Calico as Container Network Interface, and is configured with a large flat subnet (e.g. 172.30.0.0/16) for all internal application traffic inside of the cluster. Each worker node in the Kubernetes cluster is assigned one or  more non-overlapping slices of this network, coordinated by the Kubernetes master node. When a container is created in the cluster, it gets assigned to a worker node and is given an IP address from the slice of the subnet for the worker node.</p> <p></p> <p>Kube-proxy intercepts and controls where to forward the traffic, internal to the node, or to another worker node running  the destination pod, or outside of the cluster.   </p> <p>For inter-pod networking as pod has its own routable IP address there is no need for proxies or NAT. Except that with Kubernetes,  services use a completely separate set of IP addresses. The IP addresses are not known by Calico, and are not directly routable. However, they are still reachable by pods since the kube-proxy performs NAT on those IPs (NAT from svc IP@ to Pod IP @).</p> <p>Any pod can communicate with any another pod using its assigned IP address, even if it\u2019s on a different worker node.</p> <p>K8s services are used as virtual IP address, that are discovered via DNS and allow clients to reliably  discover and connect to containers.</p> <p>When using <code>nodePort</code> a dynamic published port number is generated by kubernetes.</p> <p>When a pod contains two containers, they run their own <code>cgroup</code>, but shares IPC, Network, and UTC (hostname) namespaces. This can be proven using the following commands:</p> <pre><code>oc exec -it my-two-container-pod -c side-car -- /bin/sh\nip address\nnetstat -ntlp\nhostname\n</code></pre>"},{"location":"k8s/k8s-1/#services","title":"Services","text":"<ul> <li>A Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them.  Services enable a loose coupling between dependent Pods.</li> <li>Services group provides network connection to a set of pods for other services in the cluster without exposing the actual  private IP address of each pod. </li> </ul> <p>As Pods are ephemeral in nature, resources like IP addresses allocated to it cannot be static. As an example the pods running for the BFF apps are:</p> <pre><code>$ kubectl get pods -n greencompute -o wide\nNAME                                                  READY     STATUS    RESTARTS   AGE       IP                NODE\ngc-caseportal-bc-caseportal-app-698f98d787-56qz5      1/1       Running   0          5d     192.168.130.74   172.16.40.138\ngc-caseportal-bc-caseportal-app-698f98d787-bhl5f      1/1       Running   0          5d     192.168.223.17   172.16.40.137\n</code></pre> <p>The 192.168.x.x is a flat, cluster wide, address space. As illustrated in the figure below:</p> <p></p> <p>The command shows the <code>eth0</code> interface in the container is mapped to this IP address.</p> <pre><code>  kubectl exec gc-caseportal-bc-caseportal-app-698f98d787-56qz5 -- ip addr\n  ....\n4: eth0@if16: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP\n    link/ether c2:74:08:4a:69:3f brd ff:ff:ff:ff:ff:ff\n    inet 192.168.130.74/32 scope global eth0\n</code></pre> <p>You can use Kubernetes services to make an app available to other pods inside the cluster or to expose an app to the internet by abstracting the dynamic IP address assignment.  The yaml below creates the casewebportal-svc: <code>kubectl apply -f svc.yml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: casewebportal-svc\nnamespace: greencompute\nlabels:\napp: casewebportal\nspec:\ntype: NodePort\nports:\n- port: 6100\ntargetPort: 6100\nprotocol: TCP\nname: http\nselector:\napp: casewebportal\n</code></pre> <p>By default, each Service also gets an IP address (known as ClusterIP), which is routable only inside the cluster.    The target port 6100 is visible as 31560 on the node</p> <pre><code>$ kubectl get  svc\nNAME                                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\ncasewebportal-svc                    NodePort    10.10.10.197   &lt;none&gt;        6100:31560/TCP   6d\n</code></pre> <p>A Service is backed by a group of Pods. These Pods are exposed through endpoints. The command below displays the endpoints.  The port number stays the same on all worker nodes and even to the Proxy node.</p> <pre><code>$ kubectl  get ep casewebportal-svc\nNAME                ENDPOINTS                                 AGE\ncasewebportal-svc   192.168.130.74:6100,192.168.223.17:6100   6d\n</code></pre> <p>When a Pod runs on a Node, the kubelet adds a set of environment variables for each active Service.  You can see those variables with the command:</p> <pre><code>$ kubectl exec gc-caseportal-bc-caseportal-app-698f98d787-56qz5 -- printenv\n</code></pre> <p>A Service does the load balancing while selecting the Pods for forwarding the data/traffic. It uses the label and label selector  to get access to the pods.</p> <p>Kubernetes offers a DNS cluster addon Service that automatically assigns dns names to other Services:</p> <pre><code>$ kubectl get services kube-dns --namespace=kube-system\nNAME       TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\nkube-dns   ClusterIP   10.10.10.10   &lt;none&gt;        53/UDP,53/TCP   10d\n</code></pre> <p>So we can do a <code>nslookup</code> within any pod to assess where to find the service:</p> <pre><code>kubectl exec gc-caseportal-bc-caseportal-app-698f98d787-56qz5 -- nslookup casewebportal-svc\n\nnslookup: cant resolve (null): Name does not resolve\nName:      casewebportal-svc\nAddress 1: 10.10.10.197 casewebportal-svc.greencompute.svc.cluster.local\n</code></pre> <p>You can access ClusterIP service using the cluster local proxy:</p> <ul> <li>Start the kubernetes local proxy: <code>kubectl proxy --port=8080</code></li> <li>Access the service via url like: http://localhost:8080/api/v1/proxy/namespaces/NAMESPACE/services/SERVICE-NAME:PORT-NAME/</li> </ul> <p>We can define a service without selectors, so no Endpoint object will be created, and we can map to your own endpoint via  the <code>subset.addresses.ip</code> spec. This is useful to abstract other kinds of backend component like DB server, a service in  another namespace...</p> <p>The NodePort ServiceType is useful when we want to make the services accessible from the external world.  BUT if the IP address of the VM changes, you need to take care of this problem in your client code. The end-user connects to the Worker Nodes on the specified port, which forwards the traffic to the applications running inside  the cluster. To access the application from the external world, administrators can configure a reverse proxy outside the   Kubernetes cluster and map the specific endpoint to the respective port on the Worker Nodes. See this article</p> <p>Using NGINX server as front end and load balancer, the routing can be done outside the cluster.  The following conf file may be deployed in the <code>/etc/nginx/conf.d</code> folder and will help to load balance between two pods:</p> <pre><code>upstream caseportal {\n  server 172.16.40.137:31560;\n  server 172.16.40.138:31560;\n}\nserver {\n  location / {\n  proxy_pass http://caseportal;\n  }\n}\n</code></pre> <p>Alternate is to use LoadBalancer service. With the LoadBalancer ServiceType: NodePort and ClusterIP Services are  automatically created, and the external load balancer will route to them. The Services are exposed at a static port  on each Worker Node. The Service is exposed externally using the underlying Cloud provider's load balancer feature.  If you want to directly expose a service, this is the default method. All traffic on the port you specify will be  forwarded to the service.</p> <p>Finally,the last model is by using ingress.</p>"},{"location":"k8s/k8s-1/#ingress","title":"Ingress","text":"<p>Ingress is another method (from LoadBalancer and NodePort) we can use to access our applications from the external world. It works at layer 7. The Kubernetes cluster must have an Ingress controller deployed to be able to create Ingress resources. The Ingress controller is deployed as a Docker container. Its Docker image contains a load balancer like NGInx and a controller  daemon. The controller daemon receives the desired Ingress configuration from Kubernetes, it generates an NGInx configuration file and restarts the load balancer process for changes to take effect. Ingress controller is a load balancer managed by Kubernetes. Ingress helps to decouple the routing rules from the application, we can then update our application without worrying about its external access. With Ingress, users don't connect directly to a Service. Users reach the Ingress endpoint, and, from there,  the request is forwarded to the respective Service. You can define one ingress for all the components of your application you want to expose to external world.</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\nname: web-ingress\nspec:\nrules:\n- host: myservice.mydomain.com\nhttp:\npaths:\n- path: /dashboardbff\n- backend:\nserviceName: dashboard-bff-svc\nservicePort: 8080\n- host: green.myweb.com\nhttp:\npaths:\n- backend:\nserviceName: green-service\nservicePort: 80\n</code></pre>"},{"location":"k8s/k8s-1/#service-discovery","title":"Service Discovery","text":"<p>As soon as the Pod starts on any Worker Node, the kubelet daemon running on that node adds a set of environment variables  in the Pod for all active Services. Be careful while ordering Services definitions, as the Pods will not have the environment variables   set for Services which are created after the Pods are created. The DNS add-on, creates a DNS record for each normal Service and the format is like: <code>my-svc.my-namespace.svc.cluster.local</code>  within the same namespace can reach to other services with just their name.</p> <p>The following diagram groups all those networking concepts: Ingress, service, kube-proxy and label selectors:</p> <p> </p>"},{"location":"k8s/k8s-1/#kubernetes-dns","title":"Kubernetes DNS","text":"<p>Kubelets resolve hostnames for pods through a Service named <code>kube-dns</code>. kube-dns runs as a pod in the kube-system namespace. Every Service that is created is automatically assigned a DNS name. By default, this hostname is <code>ServiceName.Namespace</code>. All Services that are in the same namespace may omit <code>Namespace</code> and just use <code>ServiceName</code>. Here is an example:</p> <p>To understand the default kubernetes DNS pod, you can run</p> <pre><code>k describe pod kube-dns-nw8gf -n kube-system\n</code></pre> <p>It watches the Kubernetes master for changes in Services and Endpoints, and maintains in-memory lookup structures to serve DNS requests. It has a static address, and kubelet passes DNS to each container with the --cluster-dns= flag. The default domain is <code>cluster.local</code>."},{"location":"k8s/k8s-1/#virtual-private-cloud","title":"Virtual Private Cloud","text":"<p>VPC delivers networking functions for cloud deployments in logically isolated segments using Software Defined Networking.</p> <p>It is a virtual networking platform tied to a tenant account. It provides logical isolation within the same tenant and other tenants.  Zones are created using high bandwidth and low latency connections (3 ms).  A VPC spans multiple zones at each region for high availability. It does not span regions. With VPC, operators can customize network topologies, sizes, IP addressing without NAT or tunnels. It supports bring your own IP address plan. The security controls are done at the subnet and server level. Some important features:</p> <ul> <li>A region has three or more zones. </li> <li>Multi-zone regions is designed to support enterprise workloads accross application, data, security and solution domains.</li> <li>Availability zone is an independent fault domains that do not share physical infrastructure. It has a latency requirement of &lt;500 micro second intra-zone and &lt;2 ms inter-zone.</li> </ul> <p>&lt;&lt; PREV &gt;&gt; NEXT</p>"},{"location":"k8s/k8s-2/","title":"Security","text":"<p>With kubernetes deployed internally, companies have control over their physical environment, network, operating system patches,  access to authentication and authorization internal registry. They control data access, access to administration console   for the platform and the workload deployed.</p> <p>As part of security is the Identity and Access Management to control access to resource and connection protocol  like OpenID. A team is a logical grouping of resources, users, and user groups.  Teams can be restricted to all resources within a namespace.  Access control gateway enforces role-based access control  (RBAC) for all registered services.</p> <p>Network segmentation to provide isolation, using DMZ VLAN for specific workload, so deploying cluster in DMZ.  And use VLAN to segment the physical network, subnets, clusters and namespaces to add more segmentation.   </p> <p> </p> <p>Segmentation will depend of the application requirements.</p> <p>For securing kubernetes network control, you will have Edge nodes that are worker nodes with only ingress workloads  and network policies.  Traffic reaches application through private network. Backend services may not be accessible  from public network. K8s network policy  specifies how pods can communicate with each other, and Calico, the IP based networking,  implements the policies (rule per IP addresses). Rules are ORe'ed, if any ingress / egress rule matches connection is allowed.  Their are scoped at namespace level.</p> <p>We can isolate application using labels. The <code>srv1</code> backend can be accessed only from the frontend</p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\nname: backend-allow\nspec:\npodSelector:\nmatchLabels:\napp: Srv1\nTier: backend\ningress:\n- from:\n- podSelector:\nmatchLabels:\napp: Srv1\nTier: frontend\n</code></pre> <p>and the db backend can only be accessed from the <code>srv1</code> backend.</p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\nname: db-allow\nspec:\npodSelector:\nmatchLabels:\napp: Srv1\nTier: db\ningress:\n- from:\n- podSelector:\nmatchLabels:\napp: Srv1\nTier: backend\n</code></pre>"},{"location":"k8s/k8s-2/#role-based-access-control","title":"Role Based Access Control","text":"<p>Add a user to a role:</p> <pre><code>oc adm policy add-cluster-role-to-user strimzi-cluster-operator-namespaced IAM#emailasuserid@something.com\n</code></pre>"},{"location":"k8s/k8s-2/#security-context-constraints","title":"Security Context Constraints","text":"<ul> <li>Security Context Constraints (SCCs) to control permissions a pod can perform and what resources it can access</li> <li>Access to existing SCC: <code>kubectl get scc</code></li> <li>Get security policies for a scc: <code>oc describe  scc privileged</code></li> <li>You can specify SCCs as resources that are handled by RBAC. This allows you to scope access to your SCCs to a certain project or to the entire cluster. Assigning users, groups, or service accounts directly to an SCC retains cluster-wide scope See explanations here</li> </ul> <p>&lt;&lt; PREV &gt;&gt; NEXT</p>"},{"location":"k8s/k8s-3/","title":"Demonstrating k8s","text":""},{"location":"k8s/k8s-3/#local-kubernetes-docker","title":"Local Kubernetes (docker)","text":"<p>Enable Kubernetes in Docker desktop. </p> <ul> <li> <p>Deploy Busybox:</p> <pre><code># under lab/local-play\nkubectl apply -f busybox.yaml\n</code></pre> </li> <li> <p>Deploy a webapp</p> <pre><code># under lab/local-play\nkubectl apply -f python-web-server.yaml\nkubectl port-forward service/python-webserver-service  8080\n</code></pre> </li> <li> <p>Deploy k8s dashboard:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n</code></pre> </li> <li> <p>Deploy postgresql</p> <pre><code>kubectl apply -f postgresql.yaml\n# Then use psql\nkubectl exec -it [pod-name] --  psql -h localhost -U admin --password -p 5432 postgresdb\n</code></pre> </li> <li> <p>Expose via a service</p> <pre><code>kubectl expose deployment guestbook --type=\"NodePort\" --port=3000\nkubectl get svc guestbook\n</code></pre> </li> <li> <p>Scale the application</p> <pre><code>kubectl scale --replicas=10 deployment guestbook\nkubectl rollout status deployment guestbook\nkubectl get pods\n</code></pre> </li> <li> <p>Update the application version</p> <pre><code>kubectl set image deployment/guestbook guestbook=ibmcom/guestbook:v2\n</code></pre> </li> <li> <p>Rollout last version</p> <pre><code>kubectl rollout undo deployment guestbook\n</code></pre> </li> <li> <p>Remove the app</p> <pre><code>kubectl delete deployment guestbook\nkubectl delete svc guestbook\n</code></pre> </li> </ul>"},{"location":"k8s/k8s-comp/","title":"Kubernetes Compendium","text":"<ul> <li>Kubernetes product documentation</li> <li>kubectl cheatsheet</li> <li>IKS</li> <li>Docker install on redhat or centos with some basic docker commands</li> <li>Kubernetes official tutorial</li> <li>Helm tutorial in garage web site</li> <li>Our CASE team troubleshooting</li> <li>CASE private cloud repo interesting for the different installations and terraform work.</li> <li>CASE Storage best practice</li> <li>CASE Backup and restore for k8s workload</li> <li>Deploying a Ceph cluster for ICP PV</li> <li>How To Run OpenVPN in a Docker Container on Ubuntu</li> <li>Calico as network layer</li> <li>Splendors and Miseries of Docker Network</li> <li>List of deployments done in the CASE solution implementations</li> <li>CLI summary in CASE repo</li> <li>Technical Dive into StatefulSets and Deployments in Kubernetes]</li> <li>Kubectl tricks</li> <li>Explore security in IBM Cloud Private Get familiar with the security capabilities of IBM Cloud\u2122 Private and compare them with the capabilities of other private and public clouds.</li> <li>Set of demo from IBM Cloud team</li> <li>Securing Containers Workload Patterns in IKS  explains how to run containers workload securely in a Kubernetes cluster in IBM Cloud Kubernetes Service (IKS) without using any gateway router or firewall appliance for security aspects.</li> <li> <p>Update the user name or password for admin in ICP</p> </li> <li> <p>Deep dive on IKS from Lionel Mace</p> </li> </ul>"},{"location":"k8s/k8s-faq/","title":"Kubernetes FAQ","text":""},{"location":"k8s/k8s-faq/#what-do-you-understand-by-kubernetes","title":"what do you understand by kubernetes?","text":"<p>k8s is open source container management / orchestration tool to deploy scaling and descaling of contrainers. Writing in GO and it is part of the cloud foundation.</p> <p>The key features: * automated scheduling to launch container on cluster nodes * self healing: identify container dying and reschedule them * automated rollouts and rollbacks * horizontal scaling and load balancing</p>"},{"location":"k8s/k8s-faq/#difference-between-deploying-an-app-on-host-versus-containers","title":"Difference between deploying an app on host versus containers?","text":"<p>With host you share OS, libraries, and apps co-exist.</p> <p>Packaging with container includes OS, config, the libraries and app binary. Containers are made of: * namespaces: process PID, network, UTS host and domain name isolation, Inter Process Communication isolation, user (container UID are isolated from OS ones), mount tables, * cgroups: Control group  is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes. * layered filesystem: some base layers (e.g linux base layers) can be shared by multiple containers. Layers are usually read-only, and containers make copy if they have to modify the layer * content: app, libraries...</p> <p>So containers are more portable.</p>"},{"location":"k8s/k8s-faq/#how-k8s-simplify-container-deployment","title":"How k8s simplify container deployment?","text":"<p>By externalizing configuration and requirements, and defining relation between container via configurations: deploymnet, service, secrets, config map... Developers focus on desired state of the application and defined configuration for k8s to manage.</p>"},{"location":"k8s/k8s-faq/#what-is-a-heapster","title":"What is a Heapster?","text":"<p>Heapster is a cluster-wide aggregator of monitoring and event data. It supports Kubernetes natively and works on all Kubernetes setups. It is deprecated. Consider using metrics-server and a third party metrics pipeline to gather Prometheus-format metrics instead.</p>"},{"location":"k8s/k8s-faq/#which-process-runs-on-kubernetes-master-node","title":"Which process runs on Kubernetes master node?","text":"<p><code>kubectl get pods --all-namespaces --field-selector=spec.nodeName=172.16.40.132</code></p> <ul> <li>Kube-apiserver</li> <li>etcd when not clustered</li> <li>prometheus node exporter</li> <li>logging ELK</li> <li>Heapster</li> <li>Calico node</li> <li>Auth API keys</li> <li>Vulnerability advisor</li> </ul>"},{"location":"k8s/k8s-faq/#how-to-do-multiple-deployment-a-day","title":"How to do multiple deployment a day?","text":"<p>Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones. By default, the maximum number of Pods that can be unavailable during the update and the maximum number of new Pods that can be created, is one. Updates are versioned and any Deployment update can be reverted to previous. The approach is to deploy the image to a docker registry and then use: <code>kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2</code> then we can see the new pods scheduled: <code>kubectl get pods</code>. To rollback to a previous use <code>kubectl rollout undo deployment/deployment-name</code></p> <p>Using helm we can do <code>helm rollback &lt;release-name&gt;</code></p>"},{"location":"k8s/k8s-faq/#what-is-the-best-practices-for-pod-to-pod-communications","title":"What is the best practices for pod to pod communications?","text":"<p>Don\u2019t specify a hostPort for a Pod unless it is absolutely necessary. When you bind a Pod to a hostPort, it limits the number of places the Pod can be scheduled, because each  combination must be unique. Services are assigned a DNS A record for a name of the form my-svc.my-namespace.svc.cluster.local. This resolves to the cluster IP of the Service. So prefer to use DNS name to support this communication."},{"location":"k8s/k8s-faq/#how-to-isolate-pod-to-pod-communication","title":"How to isolate pod to pod communication?","text":"<p>By default, pods are non-isolated; they accept traffic from any source. Use network policy resource to group pods allowed to communicate with each other and other network endpoints. You need a network solution that support network policies. Like Calico.</p> <p>A simple example of network policy with nginx.</p>"},{"location":"k8s/k8s-faq/#how-to-secure-internal-traffic","title":"How to secure internal traffic ?","text":"<p>The first answer is to use to istio to simplify the management of certificates and pod to pod communication. A more manual is to add a reverse-proxy like nginx in each pod. The SSL proxy will terminate HTTPS connection, using the same nginx config. App in container listen to localhost.  </p>"},{"location":"k8s/k8s-faq/#security-practices-to-consider","title":"Security practices to Consider","text":"<ul> <li>apply security to environments like host OS of the different nodes</li> <li>restrict access to etcd</li> <li>implement network segmentation:</li> <li>define network policy</li> <li>use continuous security Vulnerability scanning</li> <li>limit access to nodes</li> <li>define resource quota to control overusage</li> <li>use private repository and verified images</li> </ul>"},{"location":"k8s/k8s-faq/#how-rbac-works","title":"How RBAC works?","text":"<p>Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users. A role contains rules that represent a set of permissions. Two types: Role at the namespace level, or ClusterRole. There is no deny rules.</p> <p>A role binding grants the permissions defined in a role to a user or set of users. It holds a list of subjects (users, groups, or service accounts), and a reference to the role being granted</p> <p>To assess if RBAC is enabled: <code>kubectl get clusterroles | grep -i rbac</code> RBAC documentation</p>"},{"location":"k8s/k8s-faq/#how-to-do-ab-testing","title":"How to do A/B testing ?","text":"<p>A/B testing deployments consists of routing a subset of users to a new functionality under specific conditions, like HTTP headers, cookie, weight... You need Istio to support A/B testing.</p>"},{"location":"k8s/k8s-faq/#how-to-support-multiple-version-of-the-same-app","title":"How to support multiple version of the same app?","text":"<p>See A/B testing for one approach and use one of the following deployment strategies:</p> <ul> <li> <p>Blue/green release a new version alongside the old version then switch traffic at the load balancer level (example two different hostnames in the Ingress).</p> </li> <li> <p>Canary is used when releasing a new version to a subset of users, then proceed to a full rollout. It can be done using two Deployments with common pod labels. But the use of service mesh like istio will define greater control over traffic.</p> </li> <li> <p>A ramped deployment updates pods in a rolling update fashion, a secondary ReplicaSet is created with the new version of the application, then the number of replicas of the old version is decreased and the new version is increased until the correct number of replicas is reached.</p> </li> </ul> <p>kubernetes-deployment-strategies examples</p>"},{"location":"k8s/k8s-faq/#what-are-the-best-practices-around-logging","title":"What are the best practices around Logging?","text":"<p>Logs should have a separate storage and lifecycle independent of nodes, pods, or containers. This concept is called cluster-level-logging. To get container logs use one of: <pre><code>kubectl logs &lt;podname&gt;\nkubectl logs &lt;podname&gt; --previous\n</code></pre> By default, if a container restarts, the kubelet keeps one terminated container with its logs. Everything a containerized application writes to stdout and stderr is handled and redirected somewhere by a container engine to a logging driver.</p> <p>On machines with systemd, the kubelet and container runtime write to journald. f systemd is not present, they write to .log files in the /var/log directory.</p> <p>For cluster-level logging three choices: * Use a node-level logging agent that runs on every node: another container which access logs at node level and exposes logs or pushes logs to a backend. Better define a DaemonSet for that. This is the most common and encouraged approach for a Kubernetes cluster. For GKE Stackdriver Logging, or elasticsearch and kibana * Include a dedicated sidecar container for logging in an application pod. The sidecar streams application logs to its own stdout. It runs a logging agent, which is configured to pick up logs from an application container. Each sidecar container could tail a particular log file from a shared volume and then redirect the logs to its own stdout stream * Push logs directly to a backend from within an application.</p> <p>If you have an application that writes to a single file, it\u2019s generally better to set /dev/stdout as destination rather than implementing the streaming sidecar container approach.</p>"},{"location":"k8s/k8s-faq/#how-is-persistent-volume-allocated","title":"How is persistent volume allocated?","text":"<p>Before you can deploy most Kubernetes-based application, you must first create system-level storage from the underlying infrastructure. hostPath Persistence Volume uses local to the worker node filesystem. So create a host directory on all worker nodes in your cluster. <code>hostPath</code> PV could not failover to other worker node. NFS is shared between worker nodes. See this article to set up NFS on ubuntu. When crating the PV, use the key value pairs: <pre><code>server - 172.29.2.8\npath - /data/local/share\n</code></pre> Then define PVC and configure your application to access the PVC defined.</p>"},{"location":"k8s/k8s-faq/#how-to-debug-503","title":"How to debug 503?","text":"<p>See the troubleshooting here</p>"},{"location":"k8s/k8s-faq/#how-to-see-ingress-controller-is-up-and-running-in-k8s","title":"how to see ingress controller is up and running in k8s?","text":"<ul> <li>Get the list of ingress: <code>kubectl get ing -n greencompute</code></li> <li>Get a specific ingress: <code>kubectl describe ing -n greencompute gc-customer-ms-greencompute-customerms</code> <code>Name:             gc-customer-ms-greencompute-customerms Namespace:        greencompute Address:          172.16.40.131 Default backend:  default-http-backend:80 (&lt;none&gt;) Rules:   Host                 Path  Backends   ----                 ----  --------   customer.green.case                          /   gc-customer-ms-greencompute-customerms:9080 (&lt;none&gt;) Annotations: Events:   Type    Reason  Age   From                      Message   ----    ------  ----  ----                      -------   Normal  CREATE  42m   nginx-ingress-controller  Ingress greencompute/gc-customer-ms-greencompute-customerms</code></li> <li>Check the Ingress Controller Logs: <code>kubectl get pods -n kube-system</code> then <code>kubectl logs nginx-ingress-controller-gvcj5 -n kube-system</code>. Search for message with \"Event ... Kind: Ingress...\"</li> <li>Verify the nginx configuration with a command like: <code>kubectl exec -it -n kube-system nginx-ingress-controller-gvcj5  cat /etc/nginx/nginx.conf</code></li> </ul> <p>Another source of information</p>"},{"location":"k8s/k8s-faq/#what-is-calico","title":"What is Calico?","text":"<p>It uses a IP-only routing connectivity to connect containers. It does not need tunneling and virtual multicast or broadcast. It is an implementation of the Container Network Interface protocol: a plug and play networking capability with the goal to support adding and removing container easily with dynamic IP address assignment. Each container has an IP address as Node has one too. When a pod is scheduled the orchestrator calls CNI plugin, Calico, which defines the IP address to use, and persists the states in <code>etcd</code>. There is a container, called <code>felix</code>, which runs in each host and modifies the linux kernel to reference the new IP address in IP routing table. There is also a BGP component that propagated the IP address to all the node of the cluster so they can send connection to this new IP address. This is to flatten the network.</p>"},{"location":"k8s/k8s-faq/#container-network-interface","title":"Container Network Interface?","text":"<p>Apply the network as software approach to container but focusing of connecting workload (or remove it) instead of doing lower level of network configuration. Network policy for security is not in CNI.</p> <p></p> <p>CNI supports two commands: ADD and DEL that orchestrator can use to specify a container is added or removed. On the ADD command, CNI create a network interface (visible via <code>ifconfig</code>) within the network namespace and add new route (<code>route -n</code>) in the host so traffic can be routed to the container and in the container define default route to be able to get out.</p> <p></p>"},{"location":"k8s/k8s-faq/#what-is-the-number-of-master-needed-to-support-ha","title":"What is the number of master needed to support HA?","text":"<p>It is linked to the cluster size, and the expected fault tolerance which is computed as (#_of_master - 1) / 2. It should be at least 1. You must have an odd number of masters in your cluster. Majority is the number of master nodes needed for the cluster to be able to operate. For a cluster of 6 worker nodes, you need a majority of 4 and then a fault tolerance of 2.</p>"},{"location":"k8s/k8s-faq/#how-to-use-k8s-in-docker-edge","title":"How to use k8s in docker edge?","text":"<p>This tutorial to get Edge and k8s started. The following commands can help once docker + k8s are running too: <pre><code># Set the cluster - Use tls-verify=true to avoid: Unable to connect to the server: x509: certificate signed by unknown authority\n\n$ kubectl config set-cluster docker-for-desktop --server=https://kubernetes:6443 --insecure-skip-tls-verify=true\n\n# Error from server (Forbidden): services is forbidden: User \"system:anonymous\" cannot list services in the namespace \"kube-system\"\n</code></pre></p>"},{"location":"ocp/","title":"OpenShift Studies","text":"<p>OpenShift Container Platform is about developing, deploying, and running containerized applications. It is based on docker and Kubernetes and add the following features:</p> <ul> <li>Routes: represents the way external clients are able to access applications running on OpenShift.  The default OpenShift router (HAProxy) uses the HTTP header of the incoming request to determine where to proxy the connection.</li> <li>Deployment config: Represents the set of containers included in a pod, and the deployment strategies to be used.  DeploymentConfigs involve one or more ReplicationControllers, which contain a point-in-time record of the state of a DeploymentConfig as a Pod template</li> <li>CLI, REST API for administration or Web Console</li> <li>Multi tenants. You can also grant other users access to any of your projects. </li> <li>Source-to-image (S2I) is a tool for building reproducible Docker images. S2I supports incremental builds which re-use previously downloaded dependencies, and previously built artifacts. OpenShift is S2I-enabled and can use S2I as one of its build mechanisms.</li> <li> <p>Build config: Used by the OpenShift Source-to-Image (S2I) feature to build a container image from application source code stored in a Git repository</p> </li> <li> <p>OpenShift for production comes in several variants:</p> <ul> <li>OpenShift Origin: from http://OpenShift.org</li> <li>OpenShift Container Platform: integrated with RHEL and supported by RedHat. It allows for building a private or public PaaS cloud.</li> <li>OpenShift Online: multi-tenant public cloud managed by Red Hat</li> <li>OpenShift Dedicated: single-tenant container application platform hosted on Amazon Web Services (AWS) or Google Cloud Platform and managed by Red Hat.</li> </ul> </li> </ul> <p>See also my summary on k8s.</p>"},{"location":"ocp/#cncf","title":"CNCF","text":"<p>Cloud Native Computing Foundation project helps to accelerate the adoption of container, micro services, orchestrator and cloud native app.</p> <ul> <li>containerd for Container Runtime</li> <li>rkt for Container Runtime</li> <li>Kubernetes for Container Orchestration</li> <li>Linkerd or ISTIO for Service Mesh</li> <li>gRPC for Remote Procedure Call</li> <li>Container Network Interface (CNI) for Container Networking</li> <li>CoreDNS for Service Discovery</li> <li>Prometheus for Monitoring</li> <li>OpenTracing for Tracing</li> <li>Fluentd for Logging.</li> </ul>"},{"location":"ocp/#concepts","title":"Concepts","text":"<p>OpenShift is based on Kubernetes. It adds the concept of project, mapped to a k8s namespace, to govern the application access  control, resource quota and application's life cycle. Project is the top-level element for one to many applications.</p> <p>We can deploy any docker image, as soon as they are well built: such as defining the port any service is exposed on,  not needing to run specifically as the root user or other dedicated user, and which embeds a default command for running the application.</p> <p>Routes are used to expose app over HTTP. OpenShift can handle termination for secure HTTP connections, or a secure connection can be tunneled through the application,  with the application handling termination of the secure connection. Non HTTP applications can be exposed via a tunneled secure connection if the client supports  the SNI extension for a secure connection using TLS. A router (ingress controller) forwards HTTP and TLS requests to the service addresses inside the Kubernetes SDN.</p> <p>OpenShift routes are implemented by a cluster-wide router service, which runs as a containerized application in the OpenShift cluster.  The router service uses HAProxy as the default implementation.</p> <p>Red Hat OpenShift on IBM Cloud is a preconfigured OpenShift environment available for four hours at no charge. </p> <p>Note from the Red Hat OpenShift training</p>"},{"location":"ocp/#getting-started","title":"Getting started","text":"<p>Use IBM Cloud cluster feature to get an OpenShift cluster.</p> <p>Be familiar with OC cli commands.</p>"},{"location":"ocp/#collaborate","title":"Collaborate","text":"<p>User can be added to an existing project, via the View membership menu on a project. Each user can have different roles.  <code>Edit Role</code> can perform most tasks within the project, except tasks related to administration of the project.</p> <p>Remark</p> <p>State about the current login session is stored in the home directory of the local user running the <code>oc</code> command,  so user needs to logout and login to access a second cluster. </p>"},{"location":"ocp/#monitoring","title":"Monitoring","text":"<p>The approach an organization takes to Monitoring and Alerting on events of importance is one of the most important considerations for an enterprise.</p> <p>The goal of any monitoring endeavor is to provide observability of the platform and applications so that one can \"ask\" questions to spot anomalies and un-cover the cause of an unplanned issue as quickly as possible.</p>"},{"location":"ocp/#monitoring-overview-ocp48","title":"Monitoring Overview - OCP4.8","text":"<ul> <li>OpenShift Container Platform includes a preconfigured, preinstalled, and self-updating monitoring stack that provides monitoring for core platform components.</li> <li>A set of alerts are included by default that immediately notify cluster administrators about issues with a cluster. </li> <li>Default dashboards in the OpenShift Container Platform web console include visual representations of cluster metrics to help you to quickly understand the state of your cluster.</li> <li>access to third-party interfaces, such as Prometheus, Alertmanager, and Grafana.</li> <li>The OpenShift Container Platform monitoring stack is based on the Prometheus open source project and its wider ecosystem. The namespace is <code>openshift-monitoring</code></li> </ul>"},{"location":"ocp/#configuration","title":"Configuration:","text":"<ul> <li>To configure core OpenShift Container Platform monitoring components, you must create the cluster-monitoring-config ConfigMap object in the openshift-monitoring project.</li> </ul> <pre><code>oc -n openshift-monitoring get configmap cluster-monitoring-config\n# if not present\noc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n    name: cluster-monitoring-config\n    namespace: openshift-monitoring\ndata:\n    config.yaml: |\nEOF\n</code></pre> <ul> <li>To configure the components that monitor user-defined projects, you must create the user-workload-monitoring-config ConfigMap object in the openshift-user-workload-monitoring project.</li> </ul> <pre><code>oc -n openshift-user-workload-monitoring get configmap user-workload-monitoring-config\n# if not present\noc apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n    name: user-workload-monitoring-config\n    namespace: openshift-user-workload-monitoring\ndata:\n    config.yaml: |\nEOF\n</code></pre> <p>Add any key-value under this configuration for each component to configure</p> <ul> <li> <p>See the Configurable monitoring components</p> </li> <li> <p>Many of the monitoring components are deployed by using multiple pods across different nodes in the cluster to maintain high availability.</p> </li> <li>Running cluster monitoring with persistent storage means that your metrics are stored to a persistent volume (PV) and can survive a pod being restarted or recreated. See Prometheus database storage requirements</li> </ul>"},{"location":"ocp/#day-2-for-monitoring","title":"Day 2 for monitoring","text":"<ul> <li>Viewing performance data in the login dashboard</li> <li>Viewing Prometheus metrics</li> <li>Confirm the default Grafana dashboards are available</li> <li>Verify actionable alerts are flowing to the correct destination</li> </ul> <p>Access Prometheus using <code>Monitoring &gt; Metrics &gt;</code> In the query field, enter your PromQL query.</p> <p></p> <p>Access Grafana with <code>Monitoring &gt; Dashboard &gt;</code></p> <p></p> <p>Included with the Monitoring Stack are ~120 predefined alerts. The Alert Manager is available by selecting Alerts under the monitoring tab in the web console. Here are some Grafana OpenShift dashboards that could be deployed in your own namespace.</p> <p>We recommend integrating the Alert Manager and any other Monitoring tools you use with your organizations event management solution.</p> <p>OCP provides Operators for integration with third party products such as New Relic, Sysdig, and Prometheus.</p>"},{"location":"ocp/#custom-dashboard","title":"Custom dashboard","text":"<ul> <li>Create a namespace</li> <li>Install community-powered Grafana Operator</li> <li>Create a new Grafana instance.</li> <li>Connect the community supported Grafana in the my-grafana namespace to OpenShift monitoring in the openshift-monitoring namespace</li> <li>Grant grafana-serviceaccount service account the cluster-monitoring-view cluster role.</li> </ul> <pre><code>oc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount\n</code></pre> <ul> <li>The bearer token for this service account is used to authenticate access to Prometheus in the openshift-monitoring namespace.  The following command will display this token.</li> </ul> <pre><code>oc serviceaccounts get-token grafana-serviceaccount -n my-grafana\n</code></pre> <ul> <li>From the Grafana Data Source resource, press Create Instance, and navigate to the YAML view.  In the below YAML, substitute ${BEARER_TOKEN} with the output of the command above.</li> <li>Use custom Grafana dashboards or create your own.</li> </ul>"},{"location":"ocp/#read-more","title":"Read more","text":"<ul> <li>Product documentation - Enabling monitoring for user-defined projects</li> <li>Product documentation - configuring monitoring stack</li> <li>OCP Day 2 Operations</li> <li>2020 - Custom Grafana dashboards for Red Hat OpenShift Container Platform 4</li> </ul>"},{"location":"ocp/cp4i/","title":"Some notes on Cloud Pak for Integration","text":"<ul> <li>Marketing page</li> <li>Installation instructions from product doc</li> </ul> <p>Real integration project will use multiple instances of MQ, event streams, API management. Cloud Pak helps to manage n instances.</p>"},{"location":"ocp/cp4i/#common-problems-cp4i-addresses","title":"Common problems CP4I addresses","text":"<ul> <li>Keep integration while moving to the cloud</li> <li>easily connect applications and data across multiple clouds </li> <li>manage coherence while  LOBs and IT teams are independently proliferating their own integration model, capabilities and platform</li> <li>How to move backend capabilities are now being provided by SaaS provider</li> <li>Scale new volume of demand</li> </ul>"},{"location":"ocp/cp4i/#breaking-up-the-esb","title":"Breaking up the ESB","text":"<p>SOA was typically implemented using the ESB pattern which provides standardized synchronous connectivity to back-end systems typically over web services. Some challenges surface:</p> <ul> <li>ESB patterns often formed a single infrastructure for the whole enterprise, with tens or hundreds of integrations installed on a production server cluster.</li> <li>Few interfaces could be reused from one project to another, yet the creation and maintenance of interfaces was prohibitively expensive for the scope of any one project to take on.</li> <li>Cross-enterprise initiatives like SOA and its underlying ESB were hard to fund, and often that funding only applied to services that would be reusable enough to cover the cost of creation.</li> </ul> <p>The agile integration architecture supports using a lightweight integration runtimes to implement a container based integration. The drive for changes can be summarized by the need for fine grained deployment, the adoption of cloud native infrastructure, and decentralized ownership of the integration.</p> <p>Container brings the operational consistency for any type of product and component.</p>"},{"location":"ocp/cp4i/#common-services","title":"Common services","text":"<ul> <li>LDAP integration</li> <li>UI to manage all the components</li> <li>Consistent monitoring dashboard</li> <li>Seamless integration between APP Connect flow and API mgt.</li> <li>MQ connectivity in App Connect</li> <li>Common loggging, troubleshooting, and security</li> </ul> <p>The asset repository will keep a catalog of integration, APIs,... These assets do not have to be physically stored in the Asset Repository. The physical storage could be GitHub. These asset are treated as \"Read-Only\".</p>"},{"location":"ocp/cp4i/#installation","title":"Installation","text":"<p>See Knowledge Center.</p> <p>All is done via operators now:</p> <pre><code>oc get operators\n\n# ibm-apiconnect.openshift-operators                              5d20h\n# ibm-appconnect.openshift-operators                              5d20h\n# ibm-cert-manager-operator.ibm-common-services                   5d20h\n# ibm-common-service-operator.openshift-operators                 5d20h\n# ibm-commonui-operator-app.ibm-common-services                   5d19h\n# ibm-eventstreams.openshift-operators                            5d19h\n# ibm-iam-operator.ibm-common-services                            5d19h\n# ibm-ingress-nginx-operator-app.ibm-common-services              5d19h\n# ibm-integration-asset-repository.openshift-operators            5d20h\n# ibm-integration-operations-dashboard.openshift-operators        5d20h\n# ibm-integration-platform-navigator.openshift-operators          5d20h\n# ibm-licensing-operator-app.ibm-common-services                  5d19h\n# ibm-management-ingress-operator-app.ibm-common-services         5d19h\n# ibm-metering-operator-app.ibm-common-services                   5d10h\n# ibm-mongodb-operator-app.ibm-common-services                    5d19h\n# ibm-monitoring-exporters-operator-app.ibm-common-services       5d19h\n# ibm-monitoring-grafana-operator-app.ibm-common-services         5d19h\n# ibm-monitoring-prometheusext-operator-app.ibm-common-services   5d19h\n# ibm-mq.openshift-operators                                      5d20h\n# ibm-namespace-scope-operator.ibm-common-services                5d20h\n# ibm-odlm.ibm-common-services                                    5d20h\n# ibm-platform-api-operator-app.ibm-common-services               5d19h\n</code></pre>"},{"location":"ocp/cp4i/#mq","title":"MQ","text":"<ul> <li>MQ Marketing page</li> <li>MQ on cloud product tour tutorial</li> <li>Personal MQ summary</li> </ul> <p>My repos:</p> <ul> <li>Container inventory legacy app</li> <li>MQ Messaging Solution</li> <li>Store simulator with MQ producer</li> </ul>"},{"location":"ocp/faq/","title":"OpenShift FAQ","text":""},{"location":"ocp/faq/#access","title":"Access","text":"<p>To get connection details from the config map in the kube-public namespace:</p> <pre><code>kubectl get cm ibmcloud-cluster-info -n kube-public -o yaml\n</code></pre> <p>The cluster_address value for the master address, and the cluster_router_https_port for the port number.</p>"},{"location":"ocp/faq/#how-to-get-a-token-for-login","title":"How to get a token for login","text":"<p>As soon as a service account is created, two secrets are automatically added to it:</p> <ul> <li>an API token</li> <li>credentials for the OpenShift Container Registry</li> </ul> <p>To access the token you can go to the Openshift container platform web console, and select the user icon on the top right and the command: <code>copy login command</code> you should have the token.</p> <pre><code>oc login -u apikey -p I173tzup --server=https://c2-e.us-east.containers.cloud.ibm.com:21070\n</code></pre>"},{"location":"ocp/faq/#strange-message-from-login","title":"Strange message from login","text":"<p>Some <code>oc login</code> command may return a strange message: <code>error: invalid character '&lt;' looking for beginning of value</code>.  This is due to the fact that the response is a HTML page. This is a problem of server URL. The Server parameter has to correspond  to your OpenShift API server endpoint. When deploy on premise be sure to use the k8s master URL.</p>"},{"location":"ocp/faq/#login-and-push-image-to-private-registry","title":"Login and push image to private registry","text":"<p>OpenShift could manage its own image private registry service. The default name and URL is <code>docker-registry-default.apps....</code>.  See the product documentation here to install it.</p> <p>Below is the step to push a docker images</p> <ul> <li>Login to OpenShift cluster (get secure token from OpenShift console)</li> <li>If not done before add registry-viewer role to your user: <code>oc policy add-role-to-user registry-viewer $(oc whoami)</code>  and <code>oc policy add-role-to-user registry-editor $(oc whoami)</code></li> <li>Look up the internal OpenShift Docker registry address by using the following command:</li> </ul> <pre><code>kubectl get routes docker-registry -n default\n</code></pre> <ul> <li>Login to docker registry:</li> </ul> <pre><code>docker login -u john -p $(oc whoami -t) docker-registry-default.apps.green-with-envy.ocp.csplab.local\n</code></pre> <p>If you get this message: <code>Error response from daemon: Get https://docker-registry-default.apps.green-with-envy.ocp.csplab.local/v2/: x509: certificate signed by unknown authority</code>,  add the certificate to the docker client certificates:</p> <pre><code>    * Get the certificate: `oc extract -n default secrets/registry-certificates --keys=registry.crt`\n* Put the certificate in `~/.docker/certs.d/docker-registry-default.apps.green-with-envy.ocp.csplab.local` * Restart docker desktop\n</code></pre> <ul> <li>Tag the image with registry name:</li> </ul> <pre><code>docker tag ibmcase/kc-ordercommandms docker-registry-default.apps.green-with-envy.ocp.csplab.local/reefershipmentsolution/kc-ordercommandms\n</code></pre> <ul> <li>Push the image</li> </ul> <pre><code>docker push docker-registry-default.apps.green-with-envy.ocp.csplab.local/reefershipmentsolution/kc-ordercommandms\n</code></pre> <ul> <li> <p>Accessing the registry console: https://registry-console-default.apps.green-with-envy.ocp.csplab.local/</p> </li> <li> <p>Generate deployment.yaml and services.yaml from helm templates:</p> </li> </ul> <pre><code>helm template --set image.repository=docker-registry.default.svc:5000/reefershipmentsolution/kc-ordercommandms --set kafka.brokersConfigMap=kafka-brokers --set eventstreams.enabled=true --set eventstreams.apikeyConfigMap=eventstreams-apikey --set serviceAccountName=kcontainer-runtime  --namespace reefershipmentsolution --output-dir templates chart/ordercommandms\n</code></pre> <ul> <li>Refresh an existing pod with the new image using <code>oc delete &lt;deployment.yaml&gt;</code> and <code>oc apply &lt;deployment.yaml&gt;</code></li> </ul>"},{"location":"ocp/faq/#deployment","title":"Deployment","text":""},{"location":"ocp/faq/#deploy-any-docker-image","title":"Deploy any docker image","text":"<p>Just reference the docker image name from the dockerhub public repository</p> <pre><code>oc new-app busybox\n</code></pre> <p>For mongodb using a local env file to specify the different environment variables to be used for deployment</p> <pre><code>oc new-app --env-file=mongo.env --docker-image=openshift/mongodb-24-centos7\n</code></pre>"},{"location":"ocp/faq/#copy-a-file-to-an-existing-running-container","title":"Copy a file to an existing running container","text":"<pre><code># os rsync local folder to running pod\noc rsync $(pwd) my-connect-connect-54485b7896-k5lsj:/tmp\noc rsh my-connect-connect-54485b7896-k5lsj \nls /tmp\n# can copy file too\n</code></pre>"},{"location":"ocp/faq/#how-to-setup-tlsssl-certificate","title":"How to setup TLS/SSL certificate","text":"<p>The approach is to use secret and mounted volume to inject the SSL certifcate file so the app can use it to connect over TLS.</p> <p>If you have the key and certificates as remoteapptoaccess.key and remoteappaccess.crt, you may need to encode them with base64:</p> <pre><code>$ base64 remoteapptoaccess.keys\nLS0934345DE....\n$ base64 remoteapptoaccess.crt\nSUPERSECRETLONGSTRINGINBASE64FORMAT\n</code></pre> <p>Then create a TLS secret descriptor for kubernetes:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: remoteapp-tls-secret\ntype: Opaque\ndata:\nremoteapptoaccess.key: LS0934345DE...\nremoteapptoaccess.crt:\nSUPERSERCRETLONGSTRINGINBASE64FORMAT\n</code></pre> <p>If you only have the crt file, you just define the data for it.</p> <p>In the client app deployment.yaml set a mount point: (the following deployment example, is not complete, there are missing arguments linked to the app itself)</p> <pre><code>apiVersion:  apps/v1\nkind: Deployment metadata:\nlabels:\napp: clientApp\nname: clientApp\nspec:\nreplicas: 1\nspec:\ncontainers:\n- image: yourregistry/yournamespace/imagename\nname: clientapp\nvolumeMounts:\n- mountPath: \"/client/path/inside/container/ssl\"\nname: ssl-path\nreadOnly: true\nports:\n- containerPort: 80\nvolumes:\n- name: ssl-path\nsecret:\nsecretName: remoteapp-tls-secret\n</code></pre> <p>This declaration will add two files (remoteapptoaccess.key, remoteapptoaccess.crt) under the <code>/client/path/inside/container/ssl</code> folder. If the SSL certs and keys are not in the default folder expected by the application, environment variables should specify the paths.</p>"},{"location":"ocp/faq/#whats-new-in-opeshift-46","title":"What's new in OpeShift 4.6","text":"<ul> <li>installer: support disconnected env.</li> </ul> <p>Core:</p> <ul> <li>remote worker nodes: need to be in the same subnetwork. Share the control plane / supervisor. Tolerant to disruption.  </li> <li>Full stack automation (Installer Provisioned Infrastructure) installation on bare metal</li> <li>serverless eventing</li> <li>kubernetes 1.19</li> <li>Open Virtual Network (OVN): CNI network plugin</li> <li>security compliance operator.</li> <li>monitoring your own services. </li> <li>new log forwarding API (ClusterLogForwarder CRD): to elastic search, kafka, fluentd, syslog...</li> </ul>"},{"location":"ocp/faq/#project-removal-stay-in-terminating-state","title":"Project removal stay in Terminating state","text":"<ul> <li>List resources not deleted: example on the project edademo-dev</li> </ul> <pre><code> oc api-resources --verbs=list --namespaced -o name | xargs -n 1 oc get --show-kind --ignore-not-found -n edademo-dev\n</code></pre> <ul> <li>Remove for each object still present, the finalizers declaration:</li> </ul> <pre><code># Example for an argocd app which has created the project\noc patch -n edademo-dev rolebinding/edademo-dev-rolebinding --type=merge -p '{\"metadata\": {\"finalizers\":null}}'\noc patch -n edademo-dev rolebinding/argocd-admin   --type=merge -p '{\"metadata\": {\"finalizers\":null}}'\noc patch -n edademo-dev rolebinding/edit  --type=merge -p '{\"metadata\": {\"finalizers\":null}}'\n</code></pre>"},{"location":"ocp/notes-ocp-training/","title":"Notes on training","text":""},{"location":"ocp/notes-ocp-training/#do180","title":"DO180","text":"<p>student, which has the password student, root redhat Student workstation: workstation.lab.example.com http://rol.redhat.com Students also have access to a MySQL and a Nexus server hosted by either the OpenShift cluster or by AWS github.com jbcodeforce quay.io jbcodeforce</p>"},{"location":"ocp/notes-ocp-training/#container-technology","title":"Container technology","text":"<p>Difference between container applications and traditional deployments</p> <ul> <li>The major drawback to traditionally deployed software application is that the application's dependencies are entangled with the runtime environment</li> <li> <p>A traditionally deployed application must be stopped before updating the associated dependencies</p> <pre><code>* App server + deployed apps becomes a complex system to provide high availability\n</code></pre> </li> <li> <p>A container is a set of one or more processes that are isolated from the rest of the system. </p> <pre><code>* security, storage, and network isolation\n* isolate dependent libraries and run time resources\n* less resources than VM, start quickly.\n* helps with the efficiency, elasticity, and reusability of the hosted applications, and portability\n* [Open Container initiative](https://www.opencontainers.org/) to govern specifications for container runtime and image definition.\nThe Runtime Specification outlines how to run a \u201cfilesystem bundle\u201d that is unpacked on disk\n</code></pre> <ul> <li>Container image: bundle of files and metadata</li> <li>Container engine: Rocket, Drawbridge, LXC, Docker, and Podman</li> </ul> </li> <li> <p>Started in 2001 with VServer, then move to isolated process which leverages the following linux features:</p> <pre><code>* **Namespaces**: The kernel can isolate specific system resources, usually visible to all processes, by placing the resources within a namespace.\n</code></pre> <p>Namespaces can include resources like network interfaces, the process ID list, mount points, IPC resources, and the system's host name information.     * cgroups: Control groups partition sets of processes and their children into groups to manage and limit the resources they consume.     * Seccomp defines a security profile for processes, whitelisting the system calls, parameters and file descriptors they are allowed to use     * SELinux (Security-Enhanced Linux) is a mandatory access control system for processes. Protect processes from each other and to protect the host system from its running processes</p> </li> </ul>"},{"location":"ocp/notes-ocp-training/#openshift","title":"OpenShift","text":"<p>RHOCP adds the capabilities to provide a production PaaS platform such as remote management, multi tenancy, increased security, monitoring and auditing, application life-cycle management, and self-service interfaces for developers.</p> <pre><code>Username    RHT_OCP4_DEV_USER   boyerje-us\nPassword    RHT_OCP4_DEV_PASSWORD   &lt;&gt;\nAPI Endpoint    RHT_OCP4_MASTER_API https://api.ocp-na2.prod.nextcle.com:6443\nConsole Web Application     https://console-openshift-console.apps.ocp-na2.prod.nextcle.com\nCluster ID ...\n</code></pre> <p>Container images are named based on the following syntax: <code>registry_name/user_name/image_name:tag</code></p> <p>Example of images used:</p> <pre><code># login to a registry\nsudo podman login -u username -p password registry.access.redhat.com\nsudo podman run ubi7/ubi:7.7 echo \"Hello!\"\n# Apache http server\nsudo podman run -d rhscl/httpd-24-rhel7:2.4-36.8\n# Get IP address of a last container started\npodman inspect -l -f \"{{.NetworkSettings.IPAddress}}\" # mysql\nsudo podman run --name mysql-basic -v /var/local/mysql:/var/lib/mysql/data \\\n&gt; -e MYSQL_USER=user1 -e MYSQL_PASSWORD=mypa55 \\\n&gt; -e MYSQL_DATABASE=items -e MYSQL_ROOT_PASSWORD=r00tpa55 \\\n&gt; -d rhscl/mysql-57-rhel7:5.7-3.14\n# Stop and start a container\nsudo podman stop my-httpd-container\nsudo podman restart my-httpd-container\n# Send a SIGKILL\nsudo podman kill my-httpd-container\n</code></pre>"},{"location":"ocp/notes-ocp-training/#quayio","title":"Quay.io","text":"<p>Quay.io introduces several features, such as server-side image building, fine-grained access controls,  and automatic scanning of images for known vulnerabilities.</p> <p>Created an account with jbcodeforce user.</p> <pre><code># login\ndocker login quay.io\n# Commit an container as image to quay \ndocker commit b18bc52f5f3c quay.io/jbcodeforce/vaccineorderms:0.1.0\n# push\ndocker push quay.io/jbcodeforce/vaccineorderms:0.1.0\n</code></pre> <p>To configure registries for the <code>podman</code> command, you need to update the <code>/etc/containers/registries.conf</code>.  The podman search command finds images from all the registries listed in this file.</p> <pre><code>[registries.search]\nregistries = [\"registry.access.redhat.com\", \"quay.io\"]\n</code></pre> <p>Use an FQDN and port number (5000 default) to identify a registry. </p> <p>By default, Podman stores container images in the /var/lib/containers/storage/overlay-images directory.</p> <p>Existing images from the Podman local storage can be saved to a .tar file using the <code>podman save</code> command.</p> <pre><code># Retrieve the list of external files and directories that Podman mounts to the running container\nsudo podman inspect -f \"{{range .Mounts}}{{println .Destination}}{{end}}\" official-httpd\n# list of modified files in the container file system\nsudo podman diff official-httpd\n# Commit the changes to a new container image with a new name\nsudo podman commit -a 'Jerome' official-httpd do180-custom-httpd\nsudo podman save [-o FILE_NAME] IMAGE_NAME[:TAG]\nsudo podman tag quay.io/jbcodeforce/do180-custom-httpd:v1.0\nsudo podman push quay.io/jbcodeforce/do180-custom-httpd:v1.0\nsudo podman build -t NAME:TAG DIR\n# examine the content of the environment variable of a container\nsudo podman exec todoapi env\n</code></pre> <p>Red Hat Software Collections Library is the source of most container images</p>"},{"location":"ocp/notes-ocp-training/#building-app","title":"Building app","text":"<p>In OpenShift, a build is the process of creating a runnable container image from application source code.  A BuildConfig resource defines the entire build process. OpenShift can create container images from source code without the need for tools such as Docker or Podman.  Images are stored in the internal container registry.</p> <p>In an Source to Image S2I build, application source code is combined with an S2I builder image,  which is a container image containing the tools, libraries, and frameworks required to run the application.</p> <p>After an application is deployed on OpenShift, then OpenShift can rebuild and redeploy a new container image  anytime the application source code is modified.</p> <p>OpenShift generates unique webhook URLs for applications that are built from source stored in Git repositories.  Webhooks are configured on a Git repository. Based on the webhook configuration, GitHub will send a HTTP POST  request to the webhook URL, with details that include the latest commit information.  The OpenShift REST API listens for webhook notifications at this URL, and then triggers a new build automatically. You must configure your webhook to point to this unique URL.</p> <p>In the git repos use <code>Settings &gt; Webhooks</code> and then from the OpenShift build resource  detail view scroll down  to the webhooks URL</p>"},{"location":"ocp/notes-ocp-training/#deploying-containerized-applications-on-openshift","title":"Deploying Containerized Applications on OpenShift","text":"<p>Lab:</p> <pre><code># login to cluster \noc login -u ${RHT_OCP4_DEV_USER} -p ${RHT_OCP4_DEV_PASSWORD} ${RHT_OCP4_MASTER_API}\n# Create a new project named \"youruser-ocp\"\noc new-project ${RHT_OCP4_DEV_USER}-ocp\n# Create a temperature converter application written in PHP using the php:7.1 image stream tag. \n# The source code is in the Git repository at https://github.com/RedHatTraining/DO180-apps/\noc new-app php:7.1~https://github.com/RedHatTraining/DO180-apps --context-dir temps --name temps\n# Monitor progress of the build\noc logs -f bc/temps\n# Verify that the application is deployed.\noc get pods -w\n# Expose the temps service to create an external route for the application.\noc expose  svc/temps\n# Determine route URL\noc get route/temps\n</code></pre> <p>Other example for a nodejs app, the source code from git of a specific branch (devenv-versioning) and other context.</p> <pre><code>oc new-app --name demonode https://github.com/jbcodeforce/DO180-apps#devenv-versioning --context-dir express-helloworld\noc expose svc demonode\n</code></pre>"},{"location":"ocp/notes-ocp-training/#scaling","title":"Scaling","text":"<p>When scaling up an application, the OpenShift platform first deploys a new pod and then waits for the pod to be ready. Only after the new pod becomes available does the OpenShift platform configure the route to also send traffic to the new pod. </p> <p>When scaling down, OpenShift reconfigures the route to stop sending traffic to the pod, and then deletes the pod.</p> <p>If the application is maintaining session state, then it is important to keep those session data into a central store like a DB, redis or memcached.</p> <p>Databases, such as MariaDB and PostgreSQL, do not usually support running in multiple pods. </p> <p>Routes need to be configured to support sending message to different pods, via round-robin algorithm. Here is an example of yaml configuration to support this behavior:</p> <pre><code> annotations:\nhaproxy.router.openshift.io/balance: roundrobin\nhaproxy.router.openshift.io/disable_cookies: 'true'\nopenshift.io/host.generated: 'true'\n</code></pre> <p>In addition to manual scaling, OpenShift provides the Horizontal Pod Autoscaler (HPA) feature.  HPA automatically increases or decreases the number of pods depending on average CPU utilization.  Developers can also configure HPA to use custom application metrics for scaling.</p> <p>To set up autoscale, we need to use oc CLI</p> <pre><code>oc get dc\noc autoscale dc/php-scale2 --max=3 --cpu-percent=20\n</code></pre>"},{"location":"ocp/notes-ocp-training/#deploy-multi-container-app","title":"Deploy multi-container app","text":"<p>Podman uses Container Network Interface (CNI) to create a software-defined network (SDN) between all containers in the host.  Unless stated otherwise, CNI assigns a new IP address to a container when it starts.</p> <p>Each container exposes all ports to other containers in the same SDN. As such, services are readily accessible within the same network. The containers expose ports to external networks only by explicit configuration.</p> <p>Using environment variables allows you to share information between containers with Podman. However, there are still some limitations and some manual work involved in ensuring that all environment variables stay in sync, especially when working with many containers</p> <p>Any service defined on Kubernetes generates environment variables for the IP address and port number where the service is available. Kubernetes automatically injects these environment variables into the containers from pods in the same namespace</p> <p>Get the list of templates from where to create k8s resources like Secret, a Service, a PersistentVolumeClaim, and a DeploymentConfig: <code>oc get template -n openshift</code> and then from one of the template: <code>oc get template mysql-persistent -n openshift -o yaml</code>.</p> <p>With template, you can publish a new template to the OpenShift cluster so that other developers can build an application from the template.</p> <p>Get the parameters of a template: <code>process --parameters mysql-persistent -n openshift</code>.</p> <p>From the template create the app resources file and then deploy the app:</p> <pre><code># first export the template\noc get template mysql-persistent -o yaml -n openshift &gt; mysql-persistent-template.yaml\n# identify appropriate values for the template parameters and process the template\noc process -f mysql-persistent-template.yaml -p MYSQL_USER=dev -p MYSQL_PASSWORD=$P4SSD -p MYSQL_DATABASE=bank -p VOLUME_CAPACITY=10Gi | oc create -f -\n# or using new-app\noc new-app --template=mysql-persistent -p MYSQL_USER=dev -p MYSQL_PASSWORD=$P4SSD -p MYSQL_DATABASE=bank -p VOLUME_CAPACITY=10Gi\n</code></pre> <p>Lab:</p> <pre><code># login to cluster \noc login -u ${RHT_OCP4_DEV_USER} -p ${RHT_OCP4_DEV_PASSWORD} ${RHT_OCP4_MASTER_API}\n# Create a new project named \"youruser-\"\noc new-project ${RHT_OCP4_DEV_USER}-deploy\n# Build the MySQL Database image\nsudo podman build -t do180-mysql-57-rhel7 .\n# Push the MySQL image to the your Quay.io repository.\nsudo podman login quay.io -u ${RHT_OCP4_QUAY_USER}\nsudo podman tag do180-mysql-57-rhel7 quay.io/${RHT_OCP4_QUAY_USER}/do180-mysql-57-rhel7\nsudo podman push quay.io/${RHT_OCP4_QUAY_USER}/do180-mysql-57-rhel7\n</code></pre>"},{"location":"ocp/notes-ocp-training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ocp/notes-ocp-training/#s2i","title":"S2I","text":"<p>The S2I image creation process is composed of the build (The BuildConfig (BC) OpenShift resources drive it) and deploy (The DeploymentConfig resources) steps.</p> <ul> <li>retrieve the logs from a build configuration: <code>oc logs bc/&lt;application-name&gt;</code></li> <li>request a new build <code>oc start-build &lt;application-name&gt;</code></li> <li>Deployment logs: <code>oc logs dc/&lt;appname&gt;</code></li> </ul> <p>If a container could not access a file system, it may come to the container running under a specific user so we need to authorize by : <code>oc adm policy add-scc-to-user anyuid -z default</code>.</p> <ul> <li>Make sure the ownership and permissions of the directory are set according to the USER directive in the Dockerfile that was used to build the container being deployed</li> </ul> <p>To avoid file system permission issues, local folders used for container volume mounts must satisfy the following: * The user executing the container processes must be the owner of the folder * The local folder must satisfy the SELinux requirements to be used as a container volume. Assign the container_file_t group to the folder by using the semanage fcontext -a -t container_file_t  command, then refresh the permissions with the restorecon -R  command. * Run the <code>oc adm prune</code> command for an automated way to remove obsolete images and other resources."},{"location":"ocp/notes-ocp-training/#containerized-applications","title":"Containerized applications","text":"<p>To connect to a admin console of a pod, we can use: <code>oc port-forward</code> for forwarding a local port to a pod port. If the image enable remote debugging by exposing a port number, then port-forwarding, will let the IDE access the JVM to debug step by step via the Java Debug Wire Protocol (JDWP).</p> <p>Use openshift events to see the problem at a higher level: <code>oc get events</code>.</p> <p>To add tools like ping, telnet, iputils, dig ... into a pod, just mount local file to volume and directory like /bin, /sbin, /lib...</p> <p>To copy file to or from a container, <code>podman cp</code> is useful, or use exec: </p> <pre><code>sudo podman exec -i &lt;container&gt; mysql -uroot -proot &lt; /path/on/host/db.sql &lt; db.sql\n# or\nsudo podman exec -it &lt;containerName&gt; sh -c 'exec mysqldump -h\"$MYSQL_PORT_3306_TCP_ADDR\" \\\n -P\"$MYSQL_PORT_3306_TCP_PORT\"  -uroot -p\"$MYSQL_ENV_MYSQL_ROOT_PASSWORD\" items'  &gt; db_dump.sql\n</code></pre>"},{"location":"ocp/notes-ocp-training/#compendium","title":"Compendium","text":"<ul> <li>Open Container initiave</li> <li>Podman, the daemonless container engine for developing, managing, and running OCI Containers on your Linux System</li> <li>Quay image registry</li> <li>kubernetes</li> <li>openshift</li> <li>Git commands summary</li> </ul>"},{"location":"ocp/oc-cli/","title":"oc cheat sheet","text":"<p>oc is a tool written in Go to interact with an openshift cluster (one at a time). State about the current login session is stored in the home directory of the local user running the oc command.</p>"},{"location":"ocp/oc-cli/#login","title":"Login","text":"<pre><code>oc login --username collaborator --password collaborator \n</code></pre> <p>Log in to your server using a token for an existing session.</p> <pre><code>oc login --token &lt;token&gt; --server=https://&lt;&gt;.us-east.containers.cloud.ibm.com:21070\n</code></pre> <p>Get api version</p> <pre><code>oc api-versions\n</code></pre> <p>To assess who you are and get login userid: <pre><code>oc whoami\n</code></pre></p> <p>See what the current context:  <pre><code>oc whoami --show-context\n</code></pre></p> <p>verify which server you are logged into</p> <pre><code>oc whoami --show-server\n</code></pre> <p>Even if logged as a non admin user, we can still execute some command as admin, if the user is a sudoer:</p> <pre><code>oc get projects --as system:admin\n</code></pre>"},{"location":"ocp/oc-cli/#cluster","title":"Cluster","text":"<p>You can get a list of all OpenShift clusters you have ever logged into by running:</p> <pre><code>oc config get-clusters\n</code></pre> <p>Show a list of contexts for all sessions ever created. For each context listed, this will include details about the project, server and name of user, in that order</p> <pre><code>oc config get-contexts\n</code></pre> <p>Proxy the API server</p> <pre><code>oc proxy --port=8001\n\ncurl -X GET http://localhost:8001/api/v1/namespaces/myproject/pods\n</code></pre>"},{"location":"ocp/oc-cli/#persistence-volume","title":"Persistence volume","text":"<pre><code>oc get pv --as system:admin\n</code></pre> <p>Add a storage class</p>"},{"location":"ocp/oc-cli/#add-access-for-a-user-to-one-of-your-project","title":"Add access for a user to one of your project","text":"<pre><code>oc adm policy add-role-to-user view developer -n myproject\n&gt; role \"view\" added: \"developer\"\n</code></pre> <p>The role could be <code>edit | view | admin</code></p>"},{"location":"ocp/oc-cli/#add-user","title":"Add user","text":""},{"location":"ocp/oc-cli/#project-commands","title":"Project commands","text":"<p>Select a project once logged to openshift: <pre><code>oc project &lt;projectname&gt;\n</code></pre></p> <p>Get the list of projects</p> <pre><code>oc get projects\n</code></pre> <p>Get the list of supported app templates:</p> <pre><code>oc new-app -L\n</code></pre> <p>Create an app and build from a specific context directory.</p> <pre><code>oc new-app https://github.com/jbcodeforce/refarch-kc-order-ms --context-dir=order-command-ms/\n</code></pre> <p>To create a project, that is mapped to a kubernetes namespace:</p> <pre><code>$ oc new-project &lt;project_name&gt; --description=\"&lt;description&gt;\" --display-name=\"&lt;display_name&gt;\"\n</code></pre> <p>To see a list of all the resources that have been created in the project:</p> <pre><code>oc get all -o name\n</code></pre> <p>If you need to run a container from an image that needs to be run as root, then grant additional provilieges to the project:</p> <pre><code>oc adm policy add-scc-to-user anyuid -z default -n myproject --as system:admin\n</code></pre> <p>Verify the Deployment has been created: <code>oc get deploy</code></p> <p>Verify the ReplicaSet has been created: <code>oc get replicaset</code></p> <p>Verify the pods are running: <code>oc get pods</code></p>"},{"location":"ocp/oc-cli/#custom-resource-definition","title":"Custom Resource Definition","text":"<p>See this training</p> <pre><code>oc get crd\n</code></pre> <p>Get service account: Service accounts provide a flexible way to control API access without sharing a regular user\u2019s credentials. Every service account has an associated user name that can be granted roles. Default, builder, deployer are defined in each project)</p> <pre><code>oc get sa\n&gt; builder\ndefault\ndeployer\njb-kafka-cluster-entity-operator  \njb-kafka-cluster-kafka\njb-kafka-cluster-zookeeper\nstrimzi-cluster-operator\nstrimzi-topic-operator\n</code></pre> <p>As soon as a service account is created (<code>oc create sa strimzi-cluster-operator</code>), two secrets are automatically added to it:</p> <ul> <li>an API token</li> <li>credentials for the OpenShift Container Registry</li> </ul> <p>Access to the roles defined in a project <pre><code>oc get roles\n&gt; strimzi-topic-operator\n&gt; \noc get rolebindings\n</code></pre></p>"},{"location":"ocp/oc-cli/#work-with-images","title":"Work with images","text":"<p>Search a docker image and see if it is valid:</p> <pre><code>oc new-app --search openshiftkatacoda/blog-django-py\n</code></pre> <p>Deploy an image to a project and link it to github: <pre><code>oc new-app --docker-image=&lt;docker-image&gt; [--code=&lt;source&gt;] --name myapp\n</code></pre></p> <p>The image will be pulled down and stored within the internal OpenShift image registry. You can list what image stream resources have been created within a project by running the command:</p> <pre><code>oc get imagestream -o name\n</code></pre>"},{"location":"ocp/oc-cli/#expose-app","title":"Expose app","text":"<p>To expose the application created so it is available outside of the OpenShift cluster, you can run the command:</p> <pre><code>oc expose service/blog-django-py\n</code></pre> <p>To view the hostname assigned to the route created from the command line, you can run the command:</p> <pre><code>oc get route/blog-django-py\n</code></pre> <p>Or by using a label selector:  <pre><code>oc get all --selector app=blog-django-py -o name\n</code></pre></p> <p>Get detail of a resource:</p> <pre><code>oc describe route/blog-django-py\n</code></pre> <p>Deleting an application and all its resources, using label:</p> <pre><code>oc delete all --selector app=blog-django-py\n</code></pre> <p>To import an image without creating a container:  <pre><code>oc import-image openshiftkatacoda/blog-django-py --confirm\n</code></pre></p> <p>Then to deploy an instance of the application from the image stream which has been created, run the command: <pre><code>oc new-app blog-django-py --name blog-1\n</code></pre></p> <p>List what image stream resources have been created within a project by running the command:</p> <pre><code>oc get imagestream -o name\n</code></pre> <p>Create an app from the source code, and use source to image build process to deploy the app:</p> <pre><code>oc new-app python:latest~https://github.com/jbcodeforce/order-producer-python -name appname\n</code></pre> <p>other example with context directory: <pre><code>oc new-app https://github.com/jbcodeforce/refarch-kc-order-ms --context-dir=order-command-ms/\n</code></pre></p> <p>Then to track the deployment progress: <pre><code>oc logs -f bc/&lt;appname&gt;\n</code></pre> The dependencies are loaded, the build is scheduled and executed, the image is uploaded to the registry, and started. See the workflow in the diagram below.</p> <p></p> <p>The first time the application is deployed, if you want to expose it to internet do:</p> <pre><code>oc expose service/&lt;appname&gt;\n</code></pre> <p>When using source to image approach, it is possible to trigger the build via:</p> <pre><code>oc start-build app-name\n</code></pre> <p>You can use oc logs to monitor the log output as the build runs. You can also monitor the progress of any builds in a project by running the command:</p> <pre><code>oc get builds --watch\n</code></pre> <p>To trigger a binary build, without committing to github, you can use the local folder to provide the source code:</p> <pre><code>oc start-build app-name --from-dir=. --wait\n</code></pre> <p>The --wait option is supplied to indicate that the command should only return when the build has completed.</p> <p>Switch back to minishift context:</p> <pre><code>oc config use-context minishift\n</code></pre>"}]}